{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative model for next-layers prediction of entire PSC cell stack sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing dataframe with desired features/layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c\\AppData\\Local\\Temp\\ipykernel_7964\\60971937.py:4: DtypeWarning: Columns (10,22,29,31,32,35,36,40,44,45,46,48,51,54,65,84,89,90,93,98,99,100,105,108,115,118,122,123,125,130,134,138,142,143,144,146,149,152,163,166,167,171,172,173,175,178,181,192,194,225,271,272,273,277,304,315,321,325,330,331,335,336,342,348,369,371,373,374,376,380,384,387,403,405,407,409) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r'C:\\Users\\c\\OneDrive\\Documents\\PEROVSKITE PROJECT\\PerovskiteML_project\\Data\\Perovsite database query.csv')\n",
      "C:\\Users\\c\\AppData\\Local\\Temp\\ipykernel_7964\\60971937.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r'C:\\Users\\c\\OneDrive\\Documents\\PEROVSKITE PROJECT\\PerovskiteML_project\\Data\\Perovsite database query.csv')\n",
    "\n",
    "# Strip whitespace from ALL string entries\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "# Remove rows with NaN values in key columns\n",
    "df = df.dropna(subset=['Substrate_stack_sequence', 'ETL_stack_sequence', 'HTL_stack_sequence', 'Backcontact_stack_sequence', \n",
    "                       'Perovskite_composition_long_form', 'JV_default_PCE', 'Stability_PCE_end_of_experiment'])\n",
    "\n",
    "# Define columns to keep\n",
    "columns_to_keep = [\n",
    "    'Cell_stack_sequence', 'Cell_architecture',\n",
    "    'Substrate_stack_sequence',\n",
    "    'ETL_stack_sequence', 'ETL_thickness', 'ETL_additives_compounds',\n",
    "    'Perovskite_composition_a_ions', 'Perovskite_composition_a_ions_coefficients', \n",
    "    'Perovskite_composition_b_ions', 'Perovskite_composition_b_ions_coefficients',\n",
    "    'Perovskite_composition_c_ions', 'Perovskite_composition_c_ions_coefficients', \n",
    "    'Perovskite_composition_short_form', 'Perovskite_composition_long_form', 'Perovskite_composition_leadfree', 'Perovskite_composition_inorganic',\n",
    "    'Perovskite_additives_compounds', 'Perovskite_additives_concentrations', 'Perovskite_thickness', 'Perovskite_band_gap',\n",
    "    'HTL_stack_sequence', 'HTL_thickness_list', 'HTL_additives_compounds',\n",
    "    'Backcontact_stack_sequence',\n",
    "    'Encapsulation', 'Encapsulation_stack_sequence', 'JV_default_PCE',\n",
    "    'JV_default_Voc', 'JV_default_Jsc', 'JV_default_FF', 'JV_hysteresis_index', 'Stability_PCE_end_of_experiment'\n",
    "]\n",
    "\n",
    "# Keep only selected columns\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Ensure all stack sequence columns are stripped of any extra spaces inside entries\n",
    "for col in ['Substrate_stack_sequence', 'ETL_stack_sequence', 'HTL_stack_sequence', 'Backcontact_stack_sequence', 'Perovskite_composition_short_form']:\n",
    "    df[col] = df[col].str.replace(r'\\s*\\|\\s*', '|', regex=True)  # Remove spaces around '|'\n",
    "\n",
    "# Now filter for common materials\n",
    "df = df[\n",
    "    df['Substrate_stack_sequence'].isin(['SLG|FTO', 'SLG|ITO']) &  \n",
    "    df['ETL_stack_sequence'].isin(['TiO2-c|TiO2-mp', 'TiO2-c', 'PCBM-60|BCP', 'C60|BCP']) & \n",
    "    df['HTL_stack_sequence'].isin(['Spiro-MeOTAD', 'PEDOT:PSS']) &\n",
    "    df['Backcontact_stack_sequence'].isin(['Au', 'Ag', 'Al', 'Carbon']) &\n",
    "    df['Perovskite_composition_short_form'].isin(['MAPbI', 'CsFAMAPbBrI', 'FAMAPbBrI', 'CsPbBrI', 'MAPbBrI', 'FAPbI', 'CsFAPbBrI', 'CsPbBrI', 'CsPbI', 'CsFAPbI', 'MAPbBr']) &\n",
    "    df['Cell_architecture'].isin(['nip'])       \n",
    "]\n",
    "\n",
    "# Classify perovskites into single-layered or multi-layered\n",
    "ion_columns = [\n",
    "    'Perovskite_composition_a_ions', 'Perovskite_composition_a_ions_coefficients', \n",
    "    'Perovskite_composition_b_ions', 'Perovskite_composition_b_ions_coefficients',\n",
    "    'Perovskite_composition_c_ions', 'Perovskite_composition_c_ions_coefficients'\n",
    "]\n",
    "\n",
    "df['Layer Type'] = df.apply(\n",
    "    lambda row: 'Multi-layered Perovskite' if any('|' in str(row[col]) for col in ion_columns) else 'Single-layered Perovskite',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Keep only single-layered perovskites\n",
    "filtered_df = df[df['Layer Type'] == 'Single-layered Perovskite']\n",
    "\n",
    "# Create 'new cell stack' column by concatenating layers in the specified order\n",
    "filtered_df['Cleaned cell stack'] = filtered_df['Substrate_stack_sequence'].astype(str) + ', ' + \\\n",
    "                       df['ETL_stack_sequence'].astype(str) + ', ' + \\\n",
    "                       df['Perovskite_composition_long_form'].astype(str) + ', ' + \\\n",
    "                       df['HTL_stack_sequence'].astype(str) + ', ' + \\\n",
    "                       df['Backcontact_stack_sequence'].astype(str)\n",
    "\n",
    "filtered_df['Cleaned cell stack'] = filtered_df['Cleaned cell stack'].dropna().apply(lambda seq: f\"<SOS>,{seq},<EOS>\")\n",
    "\n",
    "filtered_df['Perovskite_composition_long_form'] = filtered_df['Perovskite_composition_long_form'].str.strip()\n",
    "\n",
    "# Save cleaned data\n",
    "filtered_df.to_csv('cleaned_data_for_generative.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the perovskite layers - K-NN CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for Transformer model: Define a custom embedding layer for materials\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Number of materials (including perovskite flattened vectors)\n",
    "num_materials = len(material_to_id) + len(ions)  # Add ions for flattened perovskite vector\n",
    "embedding_dim = 16  # Example embedding dimension\n",
    "\n",
    "class MaterialEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_materials, embedding_dim):\n",
    "        super(MaterialEmbeddingLayer, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_materials, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embeddings(x)\n",
    "\n",
    "# Example usage\n",
    "embedding_layer = MaterialEmbeddingLayer(num_materials, embedding_dim)\n",
    "\n",
    "# Example input: Full stack vector (without flattening)\n",
    "example_input = torch.tensor([material_to_id['Substrate1'], material_to_id['ETL1'], material_to_id['Pb'], material_to_id['HTL1'], material_to_id['Backcontact1']])\n",
    "\n",
    "embedded_output = embedding_layer(example_input)\n",
    "\n",
    "print(\"Embedded output:\", embedded_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplifying the model - LSTM\n",
    "This LSTM-based language model (built and trained using PyTorch) will be used to generate PSC cell stacks with varied amounts of randomness, sampling from materials in the different layers. It uses a language modelling approach to predict the next 'token' (material/layer) in each sequence, learning from existing cell stacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creating the model (using unique integer IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 133\n",
      "Sample vocabulary items: [(' Ag', 0), (' Au', 1), (' Carbon', 2), (' Cs0.025FA0.81MA0.15PbBr0.45I2.5', 3), (' Cs0.025FA0.825MA0.15PbBr0.45I2.55', 4), (' Cs0.02FA0.79MA0.16PbBr0.551I2.49', 5), (' Cs0.02FA0.83MA0.17PbBr0.51I2.49', 6), (' Cs0.04FA0.81MA0.14PbBr0.43I2.57', 7), (' Cs0.05FA0.75MA0.2PbBr0.3I2.7', 8), (' Cs0.05FA0.76MA0.16PbBr0.48I2.52', 9)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PSCStackDataset(Dataset):\n",
    "    def __init__(self, csv_file, column_name):\n",
    "        self.data = pd.read_csv(csv_file)[column_name].dropna().tolist()\n",
    "        self.data = [f\"<SOS>,{seq},<EOS>\" for seq in self.data]  # Ensure order and special tokens\n",
    "\n",
    "        # Build vocab\n",
    "        self.vocab, self.idx_to_stack = self.build_vocab(self.data)\n",
    "\n",
    "        # Encode dataset\n",
    "        self.encoded_data = [self.encode(seq) for seq in self.data]\n",
    "\n",
    "        # ðŸ”¹ Store allowed materials per layer position\n",
    "        self.allowed_per_position = self.get_allowed_per_position()\n",
    "\n",
    "    def build_vocab(self, sequences):\n",
    "        unique_tokens = set()\n",
    "        for seq in sequences:\n",
    "            unique_tokens.update(seq.split(','))  # Collect unique materials/tokens\n",
    "\n",
    "        # Ensure special tokens are included\n",
    "        unique_tokens.update(['<SOS>', '<EOS>'])\n",
    "\n",
    "        # Create consistent index mappings\n",
    "        stack_to_idx = {token: idx for idx, token in enumerate(sorted(unique_tokens))}\n",
    "        idx_to_stack = {idx: token for token, idx in stack_to_idx.items()}\n",
    "\n",
    "        return stack_to_idx, idx_to_stack\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        return [self.vocab[token] for token in sequence.split(',')]\n",
    "\n",
    "    def get_allowed_per_position(self):\n",
    "        \"\"\"Extracts unique materials at each layer position in the sequence.\"\"\"\n",
    "        allowed = {i: set() for i in range(7)}  # 7 positions: <SOS>, Substrate, ETL, Perovskite, HTL, Backcontact, <EOS>\n",
    "\n",
    "        for seq in self.data:\n",
    "            tokens = seq.split(',')\n",
    "            if len(tokens) == 7:  # Ensure valid sequences\n",
    "                for i, token in enumerate(tokens):\n",
    "                    allowed[i].add(token)\n",
    "\n",
    "        return {i: sorted(list(materials)) for i, materials in allowed.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.encoded_data[idx]\n",
    "        return torch.tensor(seq[:-1], dtype=torch.long), torch.tensor(seq[1:], dtype=torch.long)  # Shifted for prediction\n",
    "\n",
    "# ðŸ”¹ Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "# ðŸ”¹ Temperature sampling function\n",
    "def sample_with_temperature(probs, temperature=1.0):\n",
    "    probs = torch.pow(probs, 1.0 / temperature)\n",
    "    probs /= torch.sum(probs)\n",
    "    return torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "\n",
    "# ðŸ”¹ Hyperparameters\n",
    "EMBED_SIZE = 128\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LR = 0.001\n",
    "\n",
    "# ðŸ”¹ Load dataset\n",
    "dataset = PSCStackDataset(csv_file='cleaned_data_for_generative.csv', column_name='Cleaned cell stack')\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "df = pd.read_csv(r'cleaned_data_for_generative.csv')\n",
    "# ðŸ”¹ Initialize model\n",
    "model = LSTMModel(vocab_size=len(dataset.vocab), embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(\"Vocabulary size:\", len(dataset.vocab))\n",
    "print(\"Sample vocabulary items:\", list(dataset.vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [0/47], Loss: 4.8819\n",
      "Epoch [1/10], Batch [10/47], Loss: 2.3195\n",
      "Epoch [1/10], Batch [20/47], Loss: 1.4075\n",
      "Epoch [1/10], Batch [30/47], Loss: 1.0192\n",
      "Epoch [1/10], Batch [40/47], Loss: 0.8487\n",
      "Epoch [1/10] Finished - Average Loss: 1.7833\n",
      "Epoch [2/10], Batch [0/47], Loss: 0.6678\n",
      "Epoch [2/10], Batch [10/47], Loss: 0.5699\n",
      "Epoch [2/10], Batch [20/47], Loss: 0.5703\n",
      "Epoch [2/10], Batch [30/47], Loss: 0.5276\n",
      "Epoch [2/10], Batch [40/47], Loss: 0.4535\n",
      "Epoch [2/10] Finished - Average Loss: 0.5723\n",
      "Epoch [3/10], Batch [0/47], Loss: 0.4796\n",
      "Epoch [3/10], Batch [10/47], Loss: 0.4769\n",
      "Epoch [3/10], Batch [20/47], Loss: 0.5346\n",
      "Epoch [3/10], Batch [30/47], Loss: 0.5518\n",
      "Epoch [3/10], Batch [40/47], Loss: 0.4094\n",
      "Epoch [3/10] Finished - Average Loss: 0.4833\n",
      "Epoch [4/10], Batch [0/47], Loss: 0.4526\n",
      "Epoch [4/10], Batch [10/47], Loss: 0.4989\n",
      "Epoch [4/10], Batch [20/47], Loss: 0.5210\n",
      "Epoch [4/10], Batch [30/47], Loss: 0.4426\n",
      "Epoch [4/10], Batch [40/47], Loss: 0.4765\n",
      "Epoch [4/10] Finished - Average Loss: 0.4740\n",
      "Epoch [5/10], Batch [0/47], Loss: 0.4189\n",
      "Epoch [5/10], Batch [10/47], Loss: 0.4560\n",
      "Epoch [5/10], Batch [20/47], Loss: 0.4197\n",
      "Epoch [5/10], Batch [30/47], Loss: 0.3978\n",
      "Epoch [5/10], Batch [40/47], Loss: 0.4527\n",
      "Epoch [5/10] Finished - Average Loss: 0.4656\n",
      "Epoch [6/10], Batch [0/47], Loss: 0.4562\n",
      "Epoch [6/10], Batch [10/47], Loss: 0.3948\n",
      "Epoch [6/10], Batch [20/47], Loss: 0.4767\n",
      "Epoch [6/10], Batch [30/47], Loss: 0.4525\n",
      "Epoch [6/10], Batch [40/47], Loss: 0.5316\n",
      "Epoch [6/10] Finished - Average Loss: 0.4535\n",
      "Epoch [7/10], Batch [0/47], Loss: 0.4348\n",
      "Epoch [7/10], Batch [10/47], Loss: 0.3994\n",
      "Epoch [7/10], Batch [20/47], Loss: 0.4321\n",
      "Epoch [7/10], Batch [30/47], Loss: 0.4173\n",
      "Epoch [7/10], Batch [40/47], Loss: 0.5861\n",
      "Epoch [7/10] Finished - Average Loss: 0.4513\n",
      "Epoch [8/10], Batch [0/47], Loss: 0.4753\n",
      "Epoch [8/10], Batch [10/47], Loss: 0.3837\n",
      "Epoch [8/10], Batch [20/47], Loss: 0.4736\n",
      "Epoch [8/10], Batch [30/47], Loss: 0.4543\n",
      "Epoch [8/10], Batch [40/47], Loss: 0.3681\n",
      "Epoch [8/10] Finished - Average Loss: 0.4440\n",
      "Epoch [9/10], Batch [0/47], Loss: 0.3725\n",
      "Epoch [9/10], Batch [10/47], Loss: 0.4698\n",
      "Epoch [9/10], Batch [20/47], Loss: 0.3676\n",
      "Epoch [9/10], Batch [30/47], Loss: 0.4329\n",
      "Epoch [9/10], Batch [40/47], Loss: 0.3910\n",
      "Epoch [9/10] Finished - Average Loss: 0.4407\n",
      "Epoch [10/10], Batch [0/47], Loss: 0.4486\n",
      "Epoch [10/10], Batch [10/47], Loss: 0.4341\n",
      "Epoch [10/10], Batch [20/47], Loss: 0.4334\n",
      "Epoch [10/10], Batch [30/47], Loss: 0.3956\n",
      "Epoch [10/10], Batch [40/47], Loss: 0.4589\n",
      "Epoch [10/10] Finished - Average Loss: 0.4397\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "device = torch.device(\"cpu\")  # Keeping it on CPU\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs)  # Forward pass\n",
    "\n",
    "        # ðŸ”¹ Reshape output to match target\n",
    "        loss = criterion(outputs.view(-1, len(dataset.vocab)), targets.view(-1))\n",
    "\n",
    "        # ðŸ”¹ Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # ðŸ”¹ Optional: Print batch loss every N batches\n",
    "        if batch_idx % 10 == 0:  # Change 10 to a suitable number\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Batch [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # ðŸ”¹ Print epoch loss\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Finished - Average Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regularisation to encourage more meaningful embeddings/representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_lambda = 1e-5  # Adjust this to control embedding regularization strength\n",
    "embedding_loss = l2_lambda * model.embedding.weight.norm(2)  # L2 Regularization\n",
    "\n",
    "# Compute the main loss\n",
    "loss = criterion(outputs.view(-1, len(dataset.vocab)), targets.view(-1)) + embedding_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Generate sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sequences at Different Temperatures:\n",
      "\n",
      "Temperature 0.5:\n",
      "Sample 1: <SOS>,SLG|FTO, TiO2-c|TiO2-mp, MAPbI3, Spiro-MeOTAD, Ag\n",
      "Sample 2: <SOS>,SLG|FTO, TiO2-c, MAPbI3, Spiro-MeOTAD, Ag\n",
      "Sample 3: <SOS>,SLG|FTO, TiO2-c|TiO2-mp, MAPbI3, Spiro-MeOTAD, Au\n",
      "\n",
      "Temperature 1.0:\n",
      "Sample 1: <SOS>,SLG|FTO, TiO2-c|TiO2-mp, FA0.97MA0.03PbBr0.09I2.91, Spiro-MeOTAD, Au\n",
      "Sample 2: <SOS>,SLG|FTO, TiO2-c|TiO2-mp, CsPbBrI2, Spiro-MeOTAD, Au\n",
      "Sample 3: <SOS>,SLG|FTO, TiO2-c|TiO2-mp, Cs0.05FA0.79MA0.16PbBr0.51I2.51, Spiro-MeOTAD, Au\n",
      "\n",
      "Temperature 1.5:\n",
      "Sample 1: <SOS>,SLG|FTO, TiO2-c, MAPbI3, Spiro-MeOTAD, CsPbI3\n",
      "Sample 2: <SOS>,SLG|FTO, TiO2-c, FA0.85MA0.15PbBr0.45I2.55, Spiro-MeOTAD, Ag\n",
      "Sample 3: <SOS>,SLG|FTO, TiO2-c|TiO2-mp, FAPbI3, Spiro-MeOTAD, Au\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "def generate_sequence(model, dataset, start_token='<SOS>', max_length=7, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generates a valid perovskite solar cell stack sequence while ensuring layer constraints.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Start with <SOS> token\n",
    "    generated_sequence = []\n",
    "    \n",
    "    # Convert words to indices\n",
    "    input_seq = torch.tensor([dataset.vocab['<SOS>']], dtype=torch.long).unsqueeze(0)  # Shape: (1, 1)\n",
    "    \n",
    "    for position in range(1, max_length):  # Generate each layer position\n",
    "        allowed_tokens = dataset.allowed_per_position[position]  # Get valid choices for this layer\n",
    "        allowed_indices = [dataset.vocab[token] for token in allowed_tokens]  # Convert to vocab indices\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(input_seq)  # Unpack logits and hidden state\n",
    "            logits = output[:, -1, :]  # Get logits for the last generated token\n",
    "\n",
    "        logits = output[:, -1, :]  # Get logits for the last generated token\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        probs = F.softmax(logits, dim=-1).squeeze()\n",
    "\n",
    "        # # Debugging print statements\n",
    "        # print(f\"Logits: {logits}\")\n",
    "        # print(f\"Probabilities: {probs}\")\n",
    "\n",
    "        # Check for NaN or negative values\n",
    "        if torch.isnan(probs).any() or torch.isinf(probs).any():\n",
    "            raise ValueError(\"Found NaN or Inf in probability tensor!\")\n",
    "        if (probs < 0).any():\n",
    "            raise ValueError(\"Found negative probabilities!\")\n",
    "\n",
    "        # Sample next token\n",
    "        next_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        next_token = dataset.idx_to_stack[next_idx]\n",
    "\n",
    "        generated_sequence.append(next_token)\n",
    "        \n",
    "        if next_token == '<EOS>':\n",
    "            break  # Stop generation when <EOS> is reached\n",
    "        \n",
    "        # Append new token to input sequence\n",
    "        input_seq = torch.cat([input_seq, torch.tensor([[next_idx]], dtype=torch.long)], dim=1)\n",
    "\n",
    "    return ','.join(generated_sequence)\n",
    "\n",
    "# ðŸ”¹ Generate sequences at different temperatures\n",
    "temperatures = [0.5, 1.0, 1.5]\n",
    "\n",
    "print(\"\\nGenerated Sequences at Different Temperatures:\")\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature {temp}:\")\n",
    "    for i in range(3):  # Generate 3 sequences per temperature\n",
    "        print(f\"Sample {i+1}: {generate_sequence(model, dataset, start_token='<SOS>', max_length=7, temperature=temp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression models to predict PCE and stability of generated stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load and create embeddings, then convert the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# ðŸ“Œ Get the embedding matrix from the trained LSTM model\n",
    "embedding_layer = model.embedding  # Assuming your LSTM model has an embedding layer\n",
    "embedding_matrix = embedding_layer.weight.detach().cpu().numpy()  # Convert to NumPy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def stack_to_vector(stack, dataset, embedding_matrix, method=\"mean\"):\n",
    "    \"\"\"\n",
    "    Converts a cleaned cell stack sequence into a numerical vector.\n",
    "    \n",
    "    Args:\n",
    "        stack (str): Stack sequence (comma-separated materials).\n",
    "        dataset: Dataset object containing vocab mappings.\n",
    "        embedding_matrix (numpy.array): Pretrained material embeddings.\n",
    "        method (str): How to combine material embeddings. Options: \"mean\", \"concat\".\n",
    "        \n",
    "    Returns:\n",
    "        numpy.array: Vector representation of the stack.\n",
    "    \"\"\"\n",
    "    tokens = stack.split(',')\n",
    "    token_indices = [dataset.vocab[token] for token in tokens if token in dataset.vocab]\n",
    "    \n",
    "    if not token_indices:  # In case no valid tokens are found\n",
    "        return np.zeros(embedding_matrix.shape[1])  # Return a zero vector\n",
    "    \n",
    "    embeddings = embedding_matrix[token_indices]  # Fetch embeddings\n",
    "\n",
    "    if method == \"mean\":\n",
    "        return np.mean(embeddings, axis=0)  # Average pooling (good for variable-length stacks)\n",
    "    elif method == \"concat\":\n",
    "        # Ensure fixed length: Pad with zeros if needed\n",
    "        max_layers = 5  # Adjust based on your max stack size\n",
    "        padded_embeddings = np.zeros((max_layers, embedding_matrix.shape[1]))\n",
    "        for i, emb in enumerate(embeddings[:max_layers]):\n",
    "            padded_embeddings[i] = emb\n",
    "        return padded_embeddings.flatten()  # Concatenate embeddings\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Use 'mean' or 'concat'.\")\n",
    "\n",
    "# Convert entire dataset into vectors\n",
    "X = np.array([stack_to_vector(stack, dataset, embedding_matrix) for stack in df['Cleaned cell stack']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack: <SOS>,SLG|ITO, TiO2-c, MAPbI3, Spiro-MeOTAD, Au -> Predicted JV_default_PCE: 16.43, Stability_PCE_end_of_experiment: 48.47 %\n",
      "Stack: <SOS>,SLG|FTO, TiO2-c, CsPbBrI2, Spiro-MeOTAD, Ag -> Predicted JV_default_PCE: 12.22, Stability_PCE_end_of_experiment: 53.80 %\n",
      "Stack: <SOS>,SLG|FTO, TiO2-c, MAPbI3, Spiro-MeOTAD, Au -> Predicted JV_default_PCE: 16.41, Stability_PCE_end_of_experiment: 53.91 %\n",
      "Stack: <SOS>,SLG|FTO, TiO2-c, MAPbI3, Spiro-MeOTAD, Ag -> Predicted JV_default_PCE: 14.50, Stability_PCE_end_of_experiment: 53.26 %\n",
      "Stack: <SOS>,SLG|FTO, TiO2-c, MAPbI3, Spiro-MeOTAD, Au -> Predicted JV_default_PCE: 16.41, Stability_PCE_end_of_experiment: 53.91 %\n",
      "Stack: <SOS>,SLG|FTO, TiO2-c, CsPbBrI2, Spiro-MeOTAD, Au -> Predicted JV_default_PCE: 14.24, Stability_PCE_end_of_experiment: 61.91 %\n",
      "Stack: <SOS>,SLG|FTO, TiO2-c|TiO2-mp, MAPbI3, Spiro-MeOTAD, Au -> Predicted JV_default_PCE: 15.78, Stability_PCE_end_of_experiment: 59.07 %\n",
      "Stack: <SOS>,SLG|FTO, TiO2-c|TiO2-mp, FA0.86MA0.15PbBr0.45I2.55, Spiro-MeOTAD, Au -> Predicted JV_default_PCE: 18.87, Stability_PCE_end_of_experiment: 57.02 %\n",
      "Stack: <SOS>,SLG|FTO, TiO2-c|TiO2-mp, MAPbI3, Spiro-MeOTAD, Au -> Predicted JV_default_PCE: 15.78, Stability_PCE_end_of_experiment: 59.07 %\n",
      "Stack: <SOS>,SLG|FTO, TiO2-c|TiO2-mp, MAPbI3, Spiro-MeOTAD, Au -> Predicted JV_default_PCE: 15.78, Stability_PCE_end_of_experiment: 59.07 %\n"
     ]
    }
   ],
   "source": [
    "y_pce = df['JV_default_PCE'].values\n",
    "y_stability = df['Stability_PCE_end_of_experiment'].values\n",
    "\n",
    "# Train XGBoost models or any other regression model\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train_stability = y_train_stability.astype(np.float32)\n",
    "\n",
    "X_train, X_test, y_train_pce, y_test_pce = train_test_split(X, y_pce, test_size=0.2, random_state=42)\n",
    "\n",
    "pce_model = XGBRegressor()\n",
    "pce_model.fit(X_train, y_train_pce)\n",
    "\n",
    "stability_model = XGBRegressor()\n",
    "stability_model.fit(X_train, y_train_stability)\n",
    "\n",
    "# ðŸ”¹ Generate stacks using your trained model at temperature = 1.0\n",
    "generated_stacks = [\n",
    "    generate_sequence(model, dataset, start_token='<SOS>', max_length=7, temperature=1.0)\n",
    "    for _ in range(10)  # Generate 10 stacks\n",
    "]\n",
    "\n",
    "# ðŸ”¹ Convert generated stacks to numerical feature vectors\n",
    "generated_vectors = np.array([stack_to_vector(stack, dataset, embedding_matrix) for stack in generated_stacks])\n",
    "\n",
    "# ðŸ”¹ Predict PCE and Stability\n",
    "pred_pce = pce_model.predict(generated_vectors)\n",
    "pred_stability = stability_model.predict(generated_vectors)\n",
    "\n",
    "# ðŸ”¹ Print predictions\n",
    "for stack, pce, stability in zip(generated_stacks, pred_pce, pred_stability):\n",
    "    print(f\"Stack: {stack} -> Predicted JV_default_PCE: {pce:.2f}, Stability_PCE_end_of_experiment: {stability:.2f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compare to true values of PCE and stability of given stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Generated Stack  \\\n",
      "0    <SOS>,SLG|ITO, TiO2-c, MAPbI3, Spiro-MeOTAD, Au   \n",
      "1  <SOS>,SLG|FTO, TiO2-c, CsPbBrI2, Spiro-MeOTAD, Ag   \n",
      "2    <SOS>,SLG|FTO, TiO2-c, MAPbI3, Spiro-MeOTAD, Au   \n",
      "3    <SOS>,SLG|FTO, TiO2-c, MAPbI3, Spiro-MeOTAD, Ag   \n",
      "4    <SOS>,SLG|FTO, TiO2-c, MAPbI3, Spiro-MeOTAD, Au   \n",
      "5  <SOS>,SLG|FTO, TiO2-c, CsPbBrI2, Spiro-MeOTAD, Au   \n",
      "6  <SOS>,SLG|FTO, TiO2-c|TiO2-mp, MAPbI3, Spiro-M...   \n",
      "7  <SOS>,SLG|FTO, TiO2-c|TiO2-mp, FA0.86MA0.15PbB...   \n",
      "8  <SOS>,SLG|FTO, TiO2-c|TiO2-mp, MAPbI3, Spiro-M...   \n",
      "9  <SOS>,SLG|FTO, TiO2-c|TiO2-mp, MAPbI3, Spiro-M...   \n",
      "\n",
      "                          Generated Stack with <EOS>  Predicted PCE  \\\n",
      "0  <SOS>,SLG|ITO, TiO2-c, MAPbI3, Spiro-MeOTAD, A...      16.425735   \n",
      "1  <SOS>,SLG|FTO, TiO2-c, CsPbBrI2, Spiro-MeOTAD,...      12.221713   \n",
      "2  <SOS>,SLG|FTO, TiO2-c, MAPbI3, Spiro-MeOTAD, A...      16.405151   \n",
      "3  <SOS>,SLG|FTO, TiO2-c, MAPbI3, Spiro-MeOTAD, A...      14.498394   \n",
      "4  <SOS>,SLG|FTO, TiO2-c, MAPbI3, Spiro-MeOTAD, A...      16.405151   \n",
      "5  <SOS>,SLG|FTO, TiO2-c, CsPbBrI2, Spiro-MeOTAD,...      14.236651   \n",
      "6  <SOS>,SLG|FTO, TiO2-c|TiO2-mp, MAPbI3, Spiro-M...      15.780049   \n",
      "7  <SOS>,SLG|FTO, TiO2-c|TiO2-mp, FA0.86MA0.15PbB...      18.874811   \n",
      "8  <SOS>,SLG|FTO, TiO2-c|TiO2-mp, MAPbI3, Spiro-M...      15.780049   \n",
      "9  <SOS>,SLG|FTO, TiO2-c|TiO2-mp, MAPbI3, Spiro-M...      15.780049   \n",
      "\n",
      "   Predicted Stability  True PCE  True Stability  \n",
      "0            48.473724     14.38             0.0  \n",
      "1            53.796265     10.56            90.0  \n",
      "2            53.910728     13.30            61.0  \n",
      "3            53.260780     16.34            85.0  \n",
      "4            53.910728     13.30            61.0  \n",
      "5            61.911488     14.26            46.0  \n",
      "6            59.070599     19.10            50.0  \n",
      "7            57.021626       NaN             NaN  \n",
      "8            59.070599     19.10            50.0  \n",
      "9            59.070599     19.10            50.0  \n",
      "Mean Absolute Error (PCE): 2.42 %\n",
      "Mean Absolute Error (Stability): 19.30 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Append <EOS> to generated stacks\n",
    "generated_stacks_with_eos = [stack + \",<EOS>\" for stack in generated_stacks]\n",
    "\n",
    "# Find true PCE and stability values\n",
    "true_pce = []\n",
    "true_stability = []\n",
    "\n",
    "for stack in generated_stacks_with_eos:\n",
    "    match = df[df['Cleaned cell stack'].str.lower().str.strip() == stack.lower().strip()]   # Find exact match in dataset\n",
    "    \n",
    "    if not match.empty:\n",
    "        true_pce.append(match['JV_default_PCE'].values[0])\n",
    "        true_stability.append(match['Stability_PCE_end_of_experiment'].values[0])\n",
    "    else:\n",
    "        true_pce.append(None)\n",
    "        true_stability.append(None)\n",
    "\n",
    "# Store in DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Generated Stack\": generated_stacks,\n",
    "    \"Generated Stack with <EOS>\": generated_stacks_with_eos,\n",
    "    \"Predicted PCE\": pred_pce,\n",
    "    \"Predicted Stability\": pred_stability,\n",
    "    \"True PCE\": true_pce,\n",
    "    \"True Stability\": true_stability\n",
    "})\n",
    "\n",
    "print(comparison_df)\n",
    "\n",
    "# ðŸ”¹ Compute Mean Absolute Error (MAE) for valid entries\n",
    "valid_pce = comparison_df.dropna(subset=['True PCE'])\n",
    "valid_stability = comparison_df.dropna(subset=['True Stability'])\n",
    "\n",
    "if not valid_pce.empty:\n",
    "    pce_mae = mean_absolute_error(valid_pce['True PCE'], valid_pce['Predicted PCE'])\n",
    "    print(f\"Mean Absolute Error (PCE): {pce_mae:.2f} %\")\n",
    "\n",
    "if not valid_stability.empty:\n",
    "    stability_mae = mean_absolute_error(valid_stability['True Stability'], valid_stability['Predicted Stability'])\n",
    "    print(f\"Mean Absolute Error (Stability): {stability_mae:.2f} %\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
