{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_2661/1056806632.py\", line 3, in <module>\n",
      "    from gensim.models import Word2Vec\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/__init__.py\", line 11, in <module>\n",
      "    from gensim import parsing, corpora, matutils, interfaces, models, similarities, utils  # noqa:F401\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/parsing/__init__.py\", line 4, in <module>\n",
      "    from .preprocessing import (  # noqa:F401\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/parsing/preprocessing.py\", line 26, in <module>\n",
      "    from gensim import utils\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/utils.py\", line 35, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/sparse/__init__.py\", line 274, in <module>\n",
      "    from ._csr import *\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/sparse/_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/parsing/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"This package contains functions to preprocess raw text\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     preprocess_documents,\n\u001b[1;32m      6\u001b[0m     preprocess_string,\n\u001b[1;32m      7\u001b[0m     read_file,\n\u001b[1;32m      8\u001b[0m     read_files,\n\u001b[1;32m      9\u001b[0m     remove_stopwords,\n\u001b[1;32m     10\u001b[0m     split_alphanum,\n\u001b[1;32m     11\u001b[0m     stem_text,\n\u001b[1;32m     12\u001b[0m     strip_multiple_whitespaces,\n\u001b[1;32m     13\u001b[0m     strip_non_alphanum,\n\u001b[1;32m     14\u001b[0m     strip_numeric,\n\u001b[1;32m     15\u001b[0m     strip_punctuation,\n\u001b[1;32m     16\u001b[0m     strip_short,\n\u001b[1;32m     17\u001b[0m     strip_tags,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/parsing/preprocessing.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[1;32m     30\u001b[0m STOPWORDS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m([\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjust\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mless\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mover\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmove\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manyway\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mown\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthrough\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfifty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmill\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfind\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msomewhere\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monce\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     59\u001b[0m ])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/utils.py:35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmart_open\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mopen\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m gensim_version\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/sparse/__init__.py:274\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/sparse/_csr.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spmatrix, _array_doc_to_matrix\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _spbase, sparray\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sparsetools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[1;32m     12\u001b[0m                            get_csr_submatrix)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upcast\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compressed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cs_matrix\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from itertools import product\n",
    "import warnings\n",
    "import csv\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Load and Filter Data\n",
    "def load_and_filter_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the specified CSV file and filters rows where 'Cell_architecture' is 'nip'.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Filter rows where 'Cell_architecture' is exactly 'nip' (case-insensitive)\n",
    "    data = data[data['Cell_architecture'].str.strip().str.lower() == 'nip']\n",
    "    \n",
    "    # Reset index after filtering\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 2. Define Layer Columns\n",
    "def define_layer_columns():\n",
    "    \"\"\"\n",
    "    Defines the mapping between stack sequence columns and their corresponding layer names.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping column names to layer names.\n",
    "    \"\"\"\n",
    "    layer_columns = {\n",
    "        'Cell_stack_sequence': 'Cell',\n",
    "        'Substrate_stack_sequence': 'Substrate',\n",
    "        'ETL_stack_sequence': 'ETL',\n",
    "        'HTL_stack_sequence': 'HTL',\n",
    "        'Backcontact_stack_sequence': 'Backcontact',\n",
    "        'Add_lay_back_stack_sequence': 'Add_Lay_Back',\n",
    "        'Encapsulation_stack_sequence': 'Encapsulation'\n",
    "    }\n",
    "    return layer_columns\n",
    "\n",
    "# 3. Parse Sequences from Multiple Columns\n",
    "def parse_sequences_from_columns(dataframe, layer_columns):\n",
    "    \"\"\"\n",
    "    Parses material sequences from multiple layer-specific columns and maps materials to their layers.\n",
    "    \n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): The filtered DataFrame.\n",
    "        layer_columns (dict): Dictionary mapping column names to layer names.\n",
    "    \n",
    "    Returns:\n",
    "        list: Tokenized sequences (list of materials).\n",
    "        dict: Mapping of materials to layers with occurrence counts.\n",
    "        list: List of unique layer names.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    material_layer_map = {}  # Material to layers mapping with counts\n",
    "    layer_names = list(layer_columns.values())\n",
    "    \n",
    "    for idx, row in dataframe.iterrows():\n",
    "        sequence = []\n",
    "        for col, layer_name in layer_columns.items():\n",
    "            seq_str = row.get(col, \"\")\n",
    "            if pd.isna(seq_str) or not seq_str.strip():\n",
    "                continue\n",
    "            # Split the sequence into sub-layers if applicable\n",
    "            sub_layers = seq_str.split(' | ')\n",
    "            for sub_layer in sub_layers:\n",
    "                # Split sub-layers into materials\n",
    "                materials = [material.strip() for material in sub_layer.split('; ') if material.strip()]\n",
    "                sequence.extend(materials)\n",
    "                for material in materials:\n",
    "                    if material not in material_layer_map:\n",
    "                        material_layer_map[material] = {}\n",
    "                    if layer_name not in material_layer_map[material]:\n",
    "                        material_layer_map[material][layer_name] = 0\n",
    "                    material_layer_map[material][layer_name] += 1\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return sequences, material_layer_map, layer_names\n",
    "\n",
    "# 4. Train Word2Vec Model\n",
    "def train_word2vec(sequences, vector_size=50, window=5, min_count=1, workers=4, sg=1):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the provided material sequences.\n",
    "    \n",
    "    Parameters:\n",
    "        sequences (list): List of tokenized material sequences.\n",
    "        vector_size (int): Dimensionality of the embeddings.\n",
    "        window (int): Context window size.\n",
    "        min_count (int): Minimum frequency count of materials.\n",
    "        workers (int): Number of worker threads.\n",
    "        sg (int): Training algorithm (1 for skip-gram; otherwise CBOW).\n",
    "    \n",
    "    Returns:\n",
    "        Word2Vec: Trained Word2Vec model.\n",
    "    \"\"\"\n",
    "    model = Word2Vec(\n",
    "        sentences=sequences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 5. Assign Primary Layers to Materials\n",
    "def assign_primary_layers(material_layer_map):\n",
    "    \"\"\"\n",
    "    Assigns each material to its primary layer based on the highest occurrence.\n",
    "    \n",
    "    Parameters:\n",
    "        material_layer_map (dict): Mapping of materials to layers with occurrence counts.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of materials to their primary layer.\n",
    "    \"\"\"\n",
    "    material_primary_layer = {}\n",
    "    for material, layers in material_layer_map.items():\n",
    "        # Assign the material to the layer where it occurs most frequently\n",
    "        primary_layer = max(layers, key=layers.get)\n",
    "        material_primary_layer[material] = primary_layer\n",
    "    return material_primary_layer\n",
    "\n",
    "# 6. Assign Colors to Layers\n",
    "def assign_colors_to_layers(layer_names):\n",
    "    \"\"\"\n",
    "    Assigns distinct colors to each layer using a colormap.\n",
    "    \n",
    "    Parameters:\n",
    "        layer_names (list): List of unique layer names.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of layer names to colors.\n",
    "    \"\"\"\n",
    "    num_layers = len(layer_names)\n",
    "    cmap = cm.get_cmap('tab10', num_layers) if num_layers <= 10 else cm.get_cmap('tab20', num_layers)\n",
    "    \n",
    "    layer_colors = {}\n",
    "    for idx, layer_name in enumerate(layer_names):\n",
    "        layer_colors[layer_name] = cmap(idx)\n",
    "    return layer_colors\n",
    "\n",
    "# 7. Extract Embeddings\n",
    "def extract_embeddings(model, materials):\n",
    "    \"\"\"\n",
    "    Extracts embeddings for each material from the Word2Vec model.\n",
    "    \n",
    "    Parameters:\n",
    "        model (Word2Vec): Trained Word2Vec model.\n",
    "        materials (list): List of materials.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Array of embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = np.array([model.wv[material] for material in materials])\n",
    "    return embeddings\n",
    "\n",
    "# 8. Aggregate Embeddings for Each Sample\n",
    "def aggregate_embeddings(sequences, model, vector_size=50):\n",
    "    \"\"\"\n",
    "    Aggregates material embeddings for each sample by averaging.\n",
    "    \n",
    "    Parameters:\n",
    "        sequences (list): List of tokenized material sequences for each sample.\n",
    "        model (Word2Vec): Trained Word2Vec model.\n",
    "        vector_size (int): Dimensionality of the embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Aggregated feature matrix.\n",
    "    \"\"\"\n",
    "    aggregated_features = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) == 0:\n",
    "            aggregated_features.append(np.zeros(vector_size))\n",
    "            continue\n",
    "        vectors = [model.wv[material] for material in seq if material in model.wv]\n",
    "        if vectors:\n",
    "            aggregated = np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            aggregated = np.zeros(vector_size)\n",
    "        aggregated_features.append(aggregated)\n",
    "    return np.array(aggregated_features)\n",
    "\n",
    "# 9. Plot Embeddings with Color Coding and Labels\n",
    "def plot_embeddings_colored(embeddings_2d, materials, material_primary_layer, layer_colors, title, annotate=True):\n",
    "    \"\"\"\n",
    "    Plots the 2D embeddings with colors based on their primary layers and labels each vector.\n",
    "    \n",
    "    Parameters:\n",
    "        embeddings_2d (np.ndarray): 2D embeddings.\n",
    "        materials (list): List of materials.\n",
    "        material_primary_layer (dict): Mapping of materials to their primary layers.\n",
    "        layer_colors (dict): Mapping of layer names to colors.\n",
    "        title (str): Title of the plot.\n",
    "        annotate (bool): Whether to annotate material names on the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    # Assign colors to each material based on its primary layer\n",
    "    colors_list = [layer_colors[material_primary_layer[material]] for material in materials]\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=colors_list, alpha=0.7, edgecolors='w', linewidth=0.5)\n",
    "    \n",
    "    # Optionally annotate materials\n",
    "    if annotate:\n",
    "        for i, material in enumerate(materials):\n",
    "            plt.annotate(material, xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                         fontsize=8, alpha=0.75, ha='right', va='bottom')\n",
    "    \n",
    "    # Create legend\n",
    "    legend_handles = []\n",
    "    for layer_name, color in layer_colors.items():\n",
    "        patch = mpatches.Patch(color=color, label=layer_name)\n",
    "        legend_handles.append(patch)\n",
    "    plt.legend(handles=legend_handles, title='Layers', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Dimension 1', fontsize=14)\n",
    "    plt.ylabel('Dimension 2', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 10. Prepare Feature Matrix and Target Vector\n",
    "def prepare_features_targets(aggregated_features, dataframe, target_column='JV_default_PCE'):\n",
    "    \"\"\"\n",
    "    Prepares the feature matrix and target vector for model training.\n",
    "    \n",
    "    Parameters:\n",
    "        aggregated_features (np.ndarray): Aggregated feature matrix.\n",
    "        dataframe (pd.DataFrame): Original DataFrame.\n",
    "        target_column (str): Name of the target column.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Feature matrix.\n",
    "        np.ndarray: Target vector.\n",
    "    \"\"\"\n",
    "    X = aggregated_features\n",
    "    y = dataframe[target_column].values\n",
    "    return X, y\n",
    "\n",
    "# 11. Define Models and Hyperparameters\n",
    "def define_models_hyperparameters():\n",
    "    \"\"\"\n",
    "    Defines the models and their corresponding hyperparameters for optimization.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping model names to their scikit-learn estimator and hyperparameter grid.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'RandomForest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'min_samples_leaf': [1, 2]\n",
    "            }\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'model': GradientBoostingRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'max_depth': [3, 5],\n",
    "                'min_samples_split': [2, 5]\n",
    "            }\n",
    "        },\n",
    "        'SVR': {\n",
    "            'model': SVR(),\n",
    "            'params': {\n",
    "                'C': [1, 10],\n",
    "                'epsilon': [0.1, 0.2],\n",
    "                'kernel': ['rbf', 'linear']\n",
    "            }\n",
    "        },\n",
    "        'LinearRegression': {\n",
    "            'model': LinearRegression(),\n",
    "            'params': {\n",
    "                # Linear Regression has no hyperparameters to tune in scikit-learn\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return models\n",
    "\n",
    "# 12. Train and Evaluate Models\n",
    "def train_evaluate_models(X, y, models, cv=5):\n",
    "    \"\"\"\n",
    "    Trains and evaluates models with different hyperparameters using cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (np.ndarray): Target vector.\n",
    "        models (dict): Dictionary of models and their hyperparameters.\n",
    "        cv (int): Number of cross-validation folds.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing model details and performance metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    for model_name, config in models.items():\n",
    "        estimator = config['model']\n",
    "        param_grid = config['params']\n",
    "        \n",
    "        # If there are no hyperparameters to tune (e.g., LinearRegression)\n",
    "        if not param_grid:\n",
    "            print(f\"Training {model_name} with default parameters.\")\n",
    "            mae_list = []\n",
    "            mse_list = []\n",
    "            r2_list = []\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                estimator.fit(X_train, y_train)\n",
    "                y_pred = estimator.predict(X_test)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                mae_list.append(mae)\n",
    "                mse_list.append(mse)\n",
    "                r2_list.append(r2)\n",
    "            results.append({\n",
    "                'Model': model_name,\n",
    "                'Parameters': 'Default',\n",
    "                'MAE': np.mean(mae_list),\n",
    "                'MSE': np.mean(mse_list),\n",
    "                'R2': np.mean(r2_list)\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Generate all combinations of hyperparameters\n",
    "        keys = list(param_grid.keys())\n",
    "        values = list(param_grid.values())\n",
    "        for combo in product(*values):\n",
    "            params = dict(zip(keys, combo))\n",
    "            estimator.set_params(**params)\n",
    "            mae_list = []\n",
    "            mse_list = []\n",
    "            r2_list = []\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                estimator.fit(X_train, y_train)\n",
    "                y_pred = estimator.predict(X_test)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                mae_list.append(mae)\n",
    "                mse_list.append(mse)\n",
    "                r2_list.append(r2)\n",
    "            results.append({\n",
    "                'Model': model_name,\n",
    "                'Parameters': params,\n",
    "                'MAE': np.mean(mae_list),\n",
    "                'MSE': np.mean(mse_list),\n",
    "                'R2': np.mean(r2_list)\n",
    "            })\n",
    "            print(f\"Trained {model_name} with parameters {params} -> MAE: {np.mean(mae_list):.4f}, MSE: {np.mean(mse_list):.4f}, R2: {np.mean(r2_list):.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 13. Save Results to CSV\n",
    "def save_results_to_csv(results, filename='model_results.csv'):\n",
    "    \"\"\"\n",
    "    Saves the model training results to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        results (list): List of dictionaries containing model details and performance metrics.\n",
    "        filename (str): Name of the CSV file to save the results.\n",
    "    \"\"\"\n",
    "    keys = results[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(results)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "# 14. Main Execution Function\n",
    "def main():\n",
    "    # File path to the CSV dataset\n",
    "    file_path = 'perovskite_database_query.csv'\n",
    "    \n",
    "    # 1. Load and filter data\n",
    "    data = load_and_filter_data(file_path)\n",
    "    print(f\"Loaded data with {data.shape[0]} samples.\")\n",
    "    \n",
    "    # 2. Define layer columns and their corresponding layer names\n",
    "    layer_columns = define_layer_columns()\n",
    "    \n",
    "    # 3. Parse sequences from the specified columns\n",
    "    tokenized_sequences, material_layer_map, layer_names = parse_sequences_from_columns(data, layer_columns)\n",
    "    print(f\"Parsed sequences from columns: {list(layer_columns.keys())}\")\n",
    "    \n",
    "    # 4. Train Word2Vec model\n",
    "    model = train_word2vec(tokenized_sequences)\n",
    "    print(\"Trained Word2Vec model.\")\n",
    "    \n",
    "    # 5. Get list of unique materials\n",
    "    materials = list(model.wv.index_to_key)\n",
    "    print(f\"Number of unique materials in ETL_stack_sequence: {len(materials)}\")\n",
    "    \n",
    "    # 6. Assign primary layers to materials\n",
    "    material_primary_layer = assign_primary_layers(material_layer_map)\n",
    "    \n",
    "    # 7. Assign colors to layers\n",
    "    layer_colors = assign_colors_to_layers(layer_names)\n",
    "    \n",
    "    # 8. Extract embeddings\n",
    "    embeddings = extract_embeddings(model, materials)\n",
    "    \n",
    "    # 9. Aggregate embeddings for each sample\n",
    "    aggregated_features = aggregate_embeddings(tokenized_sequences, model, vector_size=model.vector_size)\n",
    "    print(\"Aggregated embeddings for each sample.\")\n",
    "    \n",
    "    # 10. Prepare feature matrix and target vector\n",
    "    X, y = prepare_features_targets(aggregated_features, data, target_column='JV_default_PCE')\n",
    "    print(f\"Prepared feature matrix with shape {X.shape} and target vector with shape {y.shape}.\")\n",
    "    \n",
    "    # 11. Define models and hyperparameters\n",
    "    models = define_models_hyperparameters()\n",
    "    \n",
    "    # 12. Train and evaluate models\n",
    "    print(\"Starting model training and evaluation...\")\n",
    "    results = train_evaluate_models(X, y, models, cv=5)\n",
    "    \n",
    "    # 13. Save results to CSV\n",
    "    save_results_to_csv(results, filename='model_results.csv')\n",
    "    \n",
    "    # Optional: Save Word2Vec model for future use\n",
    "    model.save(\"word2vec_model_ETL.stack.sequence.model\")\n",
    "    print(\"Word2Vec model saved as 'word2vec_model_ETL.stack.sequence.model'.\")\n",
    "    \n",
    "    # Optional: Save aggregated features and target for future use\n",
    "    # np.save('aggregated_features.npy', X)\n",
    "    # np.save('target_vector.npy', y)\n",
    "    # print(\"Aggregated features and target vector saved.\")\n",
    "    \n",
    "    # 14. Visualize Embeddings (Optional)\n",
    "    # PCA Visualization\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    reduced_embeddings_pca = pca.fit_transform(embeddings)\n",
    "    plot_embeddings_colored(\n",
    "        embeddings_2d=reduced_embeddings_pca,\n",
    "        materials=materials,\n",
    "        material_primary_layer=material_primary_layer,\n",
    "        layer_colors=layer_colors,\n",
    "        title='ETL Material Embeddings with PCA (Colored by Layers)',\n",
    "        annotate=True  # Set to True to display annotations\n",
    "    )\n",
    "    \n",
    "    # t-SNE Visualization\n",
    "    tsne = TSNE(n_components=2, perplexity=5, random_state=42, init='random', learning_rate='auto')\n",
    "    reduced_embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "    plot_embeddings_colored(\n",
    "        embeddings_2d=reduced_embeddings_tsne,\n",
    "        materials=materials,\n",
    "        material_primary_layer=material_primary_layer,\n",
    "        layer_colors=layer_colors,\n",
    "        title='ETL Material Embeddings with t-SNE (Colored by Layers)',\n",
    "        annotate=True  # Set to True to display annotations\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/herbrowan/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_2661/3398537671.py\", line 3, in <module>\n",
      "    from gensim.models import Word2Vec\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/__init__.py\", line 11, in <module>\n",
      "    from gensim import parsing, corpora, matutils, interfaces, models, similarities, utils  # noqa:F401\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/parsing/__init__.py\", line 4, in <module>\n",
      "    from .preprocessing import (  # noqa:F401\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/parsing/preprocessing.py\", line 26, in <module>\n",
      "    from gensim import utils\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/utils.py\", line 35, in <module>\n",
      "    import scipy.sparse\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/sparse/__init__.py\", line 274, in <module>\n",
      "    from ._csr import *\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/sparse/_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/parsing/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"This package contains functions to preprocess raw text\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     preprocess_documents,\n\u001b[1;32m      6\u001b[0m     preprocess_string,\n\u001b[1;32m      7\u001b[0m     read_file,\n\u001b[1;32m      8\u001b[0m     read_files,\n\u001b[1;32m      9\u001b[0m     remove_stopwords,\n\u001b[1;32m     10\u001b[0m     split_alphanum,\n\u001b[1;32m     11\u001b[0m     stem_text,\n\u001b[1;32m     12\u001b[0m     strip_multiple_whitespaces,\n\u001b[1;32m     13\u001b[0m     strip_non_alphanum,\n\u001b[1;32m     14\u001b[0m     strip_numeric,\n\u001b[1;32m     15\u001b[0m     strip_punctuation,\n\u001b[1;32m     16\u001b[0m     strip_short,\n\u001b[1;32m     17\u001b[0m     strip_tags,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/parsing/preprocessing.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[1;32m     30\u001b[0m STOPWORDS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m([\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjust\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mless\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mover\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmove\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manyway\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mown\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthrough\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfifty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmill\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfind\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msomewhere\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monce\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     59\u001b[0m ])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/utils.py:35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmart_open\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mopen\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m gensim_version\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/sparse/__init__.py:274\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/sparse/_csr.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spmatrix, _array_doc_to_matrix\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _spbase, sparray\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sparsetools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[1;32m     12\u001b[0m                            get_csr_submatrix)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upcast\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compressed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cs_matrix\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from itertools import product\n",
    "import warnings\n",
    "import csv\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Load and Filter Data\n",
    "def load_and_filter_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the specified CSV file and filters rows where 'Cell_architecture' is 'nip'.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Filter rows where 'Cell_architecture' is exactly 'nip' (case-insensitive)\n",
    "    data = data[data['Cell_architecture'].str.strip().str.lower() == 'nip']\n",
    "    \n",
    "    # Reset index after filtering\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 2. Define Layer Columns\n",
    "def define_layer_columns():\n",
    "    \"\"\"\n",
    "    Defines the mapping between stack sequence columns and their corresponding layer names.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping column names to layer names.\n",
    "    \"\"\"\n",
    "    layer_columns = {\n",
    "        'Cell_stack_sequence': 'Cell',\n",
    "        'Substrate_stack_sequence': 'Substrate',\n",
    "        'ETL_stack_sequence': 'ETL',\n",
    "        'HTL_stack_sequence': 'HTL',\n",
    "        'Backcontact_stack_sequence': 'Backcontact',\n",
    "        'Add_lay_back_stack_sequence': 'Add_Lay_Back',\n",
    "        'Encapsulation_stack_sequence': 'Encapsulation'\n",
    "    }\n",
    "    return layer_columns\n",
    "\n",
    "# 3. Parse Sequences from Multiple Columns\n",
    "def parse_sequences_from_columns(dataframe, layer_columns):\n",
    "    \"\"\"\n",
    "    Parses material sequences from multiple layer-specific columns and maps materials to their layers.\n",
    "    \n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): The filtered DataFrame.\n",
    "        layer_columns (dict): Dictionary mapping column names to layer names.\n",
    "    \n",
    "    Returns:\n",
    "        list: Tokenized sequences (list of materials).\n",
    "        dict: Mapping of materials to layers with occurrence counts.\n",
    "        list: List of unique layer names.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    material_layer_map = {}  # Material to layers mapping with counts\n",
    "    layer_names = list(layer_columns.values())\n",
    "    \n",
    "    for idx, row in dataframe.iterrows():\n",
    "        sequence = []\n",
    "        for col, layer_name in layer_columns.items():\n",
    "            seq_str = row.get(col, \"\")\n",
    "            if pd.isna(seq_str) or not seq_str.strip():\n",
    "                continue\n",
    "            # Split the sequence into sub-layers if applicable\n",
    "            sub_layers = seq_str.split(' | ')\n",
    "            for sub_layer in sub_layers:\n",
    "                # Split sub-layers into materials\n",
    "                materials = [material.strip() for material in sub_layer.split('; ') if material.strip()]\n",
    "                sequence.extend(materials)\n",
    "                for material in materials:\n",
    "                    if material not in material_layer_map:\n",
    "                        material_layer_map[material] = {}\n",
    "                    if layer_name not in material_layer_map[material]:\n",
    "                        material_layer_map[material][layer_name] = 0\n",
    "                    material_layer_map[material][layer_name] += 1\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return sequences, material_layer_map, layer_names\n",
    "\n",
    "# 4. Train Word2Vec Model\n",
    "def train_word2vec(sequences, vector_size=50, window=5, min_count=1, workers=4, sg=1):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the provided material sequences.\n",
    "    \n",
    "    Parameters:\n",
    "        sequences (list): List of tokenized material sequences.\n",
    "        vector_size (int): Dimensionality of the embeddings.\n",
    "        window (int): Context window size.\n",
    "        min_count (int): Minimum frequency count of materials.\n",
    "        workers (int): Number of worker threads.\n",
    "        sg (int): Training algorithm (1 for skip-gram; otherwise CBOW).\n",
    "    \n",
    "    Returns:\n",
    "        Word2Vec: Trained Word2Vec model.\n",
    "    \"\"\"\n",
    "    model = Word2Vec(\n",
    "        sentences=sequences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 5. Assign Primary Layers to Materials\n",
    "def assign_primary_layers(material_layer_map):\n",
    "    \"\"\"\n",
    "    Assigns each material to its primary layer based on the highest occurrence.\n",
    "    \n",
    "    Parameters:\n",
    "        material_layer_map (dict): Mapping of materials to layers with occurrence counts.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of materials to their primary layer.\n",
    "    \"\"\"\n",
    "    material_primary_layer = {}\n",
    "    for material, layers in material_layer_map.items():\n",
    "        # Assign the material to the layer where it occurs most frequently\n",
    "        primary_layer = max(layers, key=layers.get)\n",
    "        material_primary_layer[material] = primary_layer\n",
    "    return material_primary_layer\n",
    "\n",
    "# 6. Assign Colors to Layers\n",
    "def assign_colors_to_layers(layer_names):\n",
    "    \"\"\"\n",
    "    Assigns distinct colors to each layer using a colormap.\n",
    "    \n",
    "    Parameters:\n",
    "        layer_names (list): List of unique layer names.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of layer names to colors.\n",
    "    \"\"\"\n",
    "    num_layers = len(layer_names)\n",
    "    cmap = cm.get_cmap('tab10', num_layers) if num_layers <= 10 else cm.get_cmap('tab20', num_layers)\n",
    "    \n",
    "    layer_colors = {}\n",
    "    for idx, layer_name in enumerate(layer_names):\n",
    "        layer_colors[layer_name] = cmap(idx)\n",
    "    return layer_colors\n",
    "\n",
    "# 7. Extract Embeddings\n",
    "def extract_embeddings(model, materials):\n",
    "    \"\"\"\n",
    "    Extracts embeddings for each material from the Word2Vec model.\n",
    "    \n",
    "    Parameters:\n",
    "        model (Word2Vec): Trained Word2Vec model.\n",
    "        materials (list): List of materials.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Array of embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = np.array([model.wv[material] for material in materials])\n",
    "    return embeddings\n",
    "\n",
    "# 8. Aggregate Embeddings for Each Sample\n",
    "def aggregate_embeddings(sequences, model, vector_size=50):\n",
    "    \"\"\"\n",
    "    Aggregates material embeddings for each sample by averaging.\n",
    "    \n",
    "    Parameters:\n",
    "        sequences (list): List of tokenized material sequences for each sample.\n",
    "        model (Word2Vec): Trained Word2Vec model.\n",
    "        vector_size (int): Dimensionality of the embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Aggregated feature matrix.\n",
    "    \"\"\"\n",
    "    aggregated_features = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) == 0:\n",
    "            aggregated_features.append(np.zeros(vector_size))\n",
    "            continue\n",
    "        vectors = [model.wv[material] for material in seq if material in model.wv]\n",
    "        if vectors:\n",
    "            aggregated = np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            aggregated = np.zeros(vector_size)\n",
    "        aggregated_features.append(aggregated)\n",
    "    return np.array(aggregated_features)\n",
    "\n",
    "# 9. Plot Embeddings with Color Coding and Labels\n",
    "def plot_embeddings_colored(embeddings_2d, materials, material_primary_layer, layer_colors, title, annotate=True):\n",
    "    \"\"\"\n",
    "    Plots the 2D embeddings with colors based on their primary layers and labels each vector.\n",
    "    \n",
    "    Parameters:\n",
    "        embeddings_2d (np.ndarray): 2D embeddings.\n",
    "        materials (list): List of materials.\n",
    "        material_primary_layer (dict): Mapping of materials to their primary layers.\n",
    "        layer_colors (dict): Mapping of layer names to colors.\n",
    "        title (str): Title of the plot.\n",
    "        annotate (bool): Whether to annotate material names on the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    # Assign colors to each material based on its primary layer\n",
    "    colors_list = [layer_colors[material_primary_layer[material]] for material in materials]\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=colors_list, alpha=0.7, edgecolors='w', linewidth=0.5)\n",
    "    \n",
    "    # Optionally annotate materials\n",
    "    if annotate:\n",
    "        for i, material in enumerate(materials):\n",
    "            plt.annotate(material, xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                         fontsize=8, alpha=0.75, ha='right', va='bottom')\n",
    "    \n",
    "    # Create legend\n",
    "    legend_handles = []\n",
    "    for layer_name, color in layer_colors.items():\n",
    "        patch = mpatches.Patch(color=color, label=layer_name)\n",
    "        legend_handles.append(patch)\n",
    "    plt.legend(handles=legend_handles, title='Layers', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Dimension 1', fontsize=14)\n",
    "    plt.ylabel('Dimension 2', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 10. Prepare Feature Matrix and Target Vector\n",
    "def prepare_features_targets(aggregated_features, dataframe, target_column='JV_default_PCE'):\n",
    "    \"\"\"\n",
    "    Prepares the feature matrix and target vector for model training.\n",
    "    Handles missing values in the target by removing corresponding samples.\n",
    "    \n",
    "    Parameters:\n",
    "        aggregated_features (np.ndarray): Aggregated feature matrix.\n",
    "        dataframe (pd.DataFrame): Original DataFrame.\n",
    "        target_column (str): Name of the target column.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Feature matrix after handling missing targets.\n",
    "        np.ndarray: Target vector after handling missing targets.\n",
    "    \"\"\"\n",
    "    # Combine features and target into a DataFrame for easier handling\n",
    "    feature_df = pd.DataFrame(aggregated_features)\n",
    "    target_series = dataframe[target_column]\n",
    "    \n",
    "    # Concatenate features and target\n",
    "    combined_df = pd.concat([feature_df, target_series], axis=1)\n",
    "    \n",
    "    # Drop rows where target is NaN\n",
    "    initial_count = combined_df.shape[0]\n",
    "    combined_df = combined_df.dropna(subset=[target_column])\n",
    "    final_count = combined_df.shape[0]\n",
    "    dropped = initial_count - final_count\n",
    "    if dropped > 0:\n",
    "        print(f\"Dropped {dropped} samples due to NaN in target '{target_column}'.\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = combined_df.drop(columns=[target_column]).values\n",
    "    y = combined_df[target_column].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 11. Define Models and Hyperparameters\n",
    "def define_models_hyperparameters():\n",
    "    \"\"\"\n",
    "    Defines the models and their corresponding hyperparameters for optimization.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping model names to their scikit-learn estimator and hyperparameter grid.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'RandomForest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'min_samples_leaf': [1, 2]\n",
    "            }\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'model': GradientBoostingRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'max_depth': [3, 5],\n",
    "                'min_samples_split': [2, 5]\n",
    "            }\n",
    "        },\n",
    "        'SVR': {\n",
    "            'model': SVR(),\n",
    "            'params': {\n",
    "                'C': [1, 10],\n",
    "                'epsilon': [0.1, 0.2],\n",
    "                'kernel': ['rbf', 'linear']\n",
    "            }\n",
    "        },\n",
    "        'LinearRegression': {\n",
    "            'model': LinearRegression(),\n",
    "            'params': {\n",
    "                # Linear Regression has no hyperparameters to tune in scikit-learn\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return models\n",
    "\n",
    "# 12. Train and Evaluate Models\n",
    "def train_evaluate_models(X, y, models, cv=5):\n",
    "    \"\"\"\n",
    "    Trains and evaluates models with different hyperparameters using cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (np.ndarray): Target vector.\n",
    "        models (dict): Dictionary of models and their hyperparameters.\n",
    "        cv (int): Number of cross-validation folds.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing model details and performance metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    for model_name, config in models.items():\n",
    "        estimator = config['model']\n",
    "        param_grid = config['params']\n",
    "        \n",
    "        # If there are no hyperparameters to tune (e.g., LinearRegression)\n",
    "        if not param_grid:\n",
    "            print(f\"Training {model_name} with default parameters.\")\n",
    "            mae_list = []\n",
    "            mse_list = []\n",
    "            r2_list = []\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                estimator.fit(X_train, y_train)\n",
    "                y_pred = estimator.predict(X_test)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                mae_list.append(mae)\n",
    "                mse_list.append(mse)\n",
    "                r2_list.append(r2)\n",
    "            results.append({\n",
    "                'Model': model_name,\n",
    "                'Parameters': 'Default',\n",
    "                'MAE': np.mean(mae_list),\n",
    "                'MSE': np.mean(mse_list),\n",
    "                'R2': np.mean(r2_list)\n",
    "            })\n",
    "            print(f\"{model_name} -> MAE: {np.mean(mae_list):.4f}, MSE: {np.mean(mse_list):.4f}, R2: {np.mean(r2_list):.4f}\")\n",
    "            continue\n",
    "        \n",
    "        # Generate all combinations of hyperparameters\n",
    "        keys = list(param_grid.keys())\n",
    "        values = list(param_grid.values())\n",
    "        for combo in product(*values):\n",
    "            params = dict(zip(keys, combo))\n",
    "            estimator.set_params(**params)\n",
    "            mae_list = []\n",
    "            mse_list = []\n",
    "            r2_list = []\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                estimator.fit(X_train, y_train)\n",
    "                y_pred = estimator.predict(X_test)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                mae_list.append(mae)\n",
    "                mse_list.append(mse)\n",
    "                r2_list.append(r2)\n",
    "            results.append({\n",
    "                'Model': model_name,\n",
    "                'Parameters': params,\n",
    "                'MAE': np.mean(mae_list),\n",
    "                'MSE': np.mean(mse_list),\n",
    "                'R2': np.mean(r2_list)\n",
    "            })\n",
    "            print(f\"Trained {model_name} with parameters {params} -> MAE: {np.mean(mae_list):.4f}, MSE: {np.mean(mse_list):.4f}, R2: {np.mean(r2_list):.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 13. Save Results to CSV\n",
    "def save_results_to_csv(results, filename='model_results.csv'):\n",
    "    \"\"\"\n",
    "    Saves the model training results to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        results (list): List of dictionaries containing model details and performance metrics.\n",
    "        filename (str): Name of the CSV file to save the results.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to save.\")\n",
    "        return\n",
    "    keys = results[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(results)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "# 14. Main Execution Function\n",
    "def main():\n",
    "    # File path to the CSV dataset\n",
    "    file_path = 'perovskite_database_query.csv'\n",
    "    \n",
    "    # 1. Load and filter data\n",
    "    data = load_and_filter_data(file_path)\n",
    "    print(f\"Loaded data with {data.shape[0]} samples.\")\n",
    "    \n",
    "    # 2. Define layer columns and their corresponding layer names\n",
    "    layer_columns = define_layer_columns()\n",
    "    \n",
    "    # 3. Parse sequences from the specified columns\n",
    "    tokenized_sequences, material_layer_map, layer_names = parse_sequences_from_columns(data, layer_columns)\n",
    "    print(f\"Parsed sequences from columns: {list(layer_columns.keys())}\")\n",
    "    \n",
    "    # 4. Train Word2Vec model\n",
    "    model = train_word2vec(tokenized_sequences)\n",
    "    print(\"Trained Word2Vec model.\")\n",
    "    \n",
    "    # 5. Get list of unique materials\n",
    "    materials = list(model.wv.index_to_key)\n",
    "    print(f\"Number of unique materials in specified stack sequences: {len(materials)}\")\n",
    "    \n",
    "    # 6. Assign primary layers to materials\n",
    "    material_primary_layer = assign_primary_layers(material_layer_map)\n",
    "    \n",
    "    # 7. Assign colors to layers\n",
    "    layer_colors = assign_colors_to_layers(layer_names)\n",
    "    \n",
    "    # 8. Extract embeddings\n",
    "    embeddings = extract_embeddings(model, materials)\n",
    "    \n",
    "    # 9. Aggregate embeddings for each sample\n",
    "    aggregated_features = aggregate_embeddings(tokenized_sequences, model, vector_size=model.vector_size)\n",
    "    print(\"Aggregated embeddings for each sample.\")\n",
    "    \n",
    "    # 10. Prepare feature matrix and target vector (Handle Missing Targets)\n",
    "    X, y = prepare_features_targets(aggregated_features, data, target_column='JV_default_PCE')\n",
    "    print(f\"Prepared feature matrix with shape {X.shape} and target vector with shape {y.shape}.\")\n",
    "    \n",
    "    # 11. Define models and hyperparameters\n",
    "    models = define_models_hyperparameters()\n",
    "    \n",
    "    # 12. Train and evaluate models\n",
    "    print(\"Starting model training and evaluation...\")\n",
    "    results = train_evaluate_models(X, y, models, cv=5)\n",
    "    \n",
    "    # 13. Save results to CSV\n",
    "    save_results_to_csv(results, filename='model_results.csv')\n",
    "    \n",
    "    # Optional: Save Word2Vec model for future use\n",
    "    model.save(\"word2vec_model_full_stack_sequence.model\")\n",
    "    print(\"Word2Vec model saved as 'word2vec_model_full_stack_sequence.model'.\")\n",
    "    \n",
    "    # Optional: Save aggregated features and target for future use\n",
    "    # np.save('aggregated_features.npy', X)\n",
    "    # np.save('target_vector.npy', y)\n",
    "    # print(\"Aggregated features and target vector saved.\")\n",
    "    \n",
    "    # 14. Visualize Embeddings (Optional)\n",
    "    # PCA Visualization\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    reduced_embeddings_pca = pca.fit_transform(embeddings)\n",
    "    plot_embeddings_colored(\n",
    "        embeddings_2d=reduced_embeddings_pca,\n",
    "        materials=materials,\n",
    "        material_primary_layer=material_primary_layer,\n",
    "        layer_colors=layer_colors,\n",
    "        title='Material Embeddings with PCA (Colored by Layers)',\n",
    "        annotate=True  # Set to True to display annotations\n",
    "    )\n",
    "    \n",
    "    # t-SNE Visualization\n",
    "    tsne = TSNE(n_components=2, perplexity=5, random_state=42, init='random', learning_rate='auto')\n",
    "    reduced_embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "    plot_embeddings_colored(\n",
    "        embeddings_2d=reduced_embeddings_tsne,\n",
    "        materials=materials,\n",
    "        material_primary_layer=material_primary_layer,\n",
    "        layer_colors=layer_colors,\n",
    "        title='Material Embeddings with t-SNE (Colored by Layers)',\n",
    "        annotate=True  # Set to True to display annotations\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data with 29560 samples.\n",
      "Parsed sequences from columns: ['Cell_stack_sequence', 'Substrate_stack_sequence', 'ETL_stack_sequence', 'HTL_stack_sequence', 'Backcontact_stack_sequence', 'Add_lay_back_stack_sequence', 'Encapsulation_stack_sequence']\n",
      "Trained Word2Vec model.\n",
      "Number of unique materials in specified stack sequences: 2149\n",
      "Aggregated embeddings for each sample.\n",
      "Dropped 624 samples due to NaN in target 'JV_default_PCE'.\n",
      "Prepared feature matrix with shape (28936, 50) and target vector with shape (28936,).\n",
      "Starting model training and evaluation...\n",
      "Training RandomForest...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 479\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# # 14. Visualize Embeddings (Optional)\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# # PCA Visualization\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# pca = PCA(n_components=2, random_state=42)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m#     annotate=False  # Set to True to display annotations\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 479\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 439\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# 12. Train and evaluate models using GridSearchCV\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting model training and evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 439\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_evaluate_models_with_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;66;03m# 13. Save results to CSV\u001b[39;00m\n\u001b[1;32m    442\u001b[0m save_results_to_csv(results, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 343\u001b[0m, in \u001b[0;36mtrain_evaluate_models_with_grid_search\u001b[0;34m(X, y, models, cv)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    334\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m    335\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    336\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    341\u001b[0m )\n\u001b[0;32m--> 343\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[1;32m    346\u001b[0m best_score \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_score_\n",
      "File \u001b[0;32m~/Documents/Documents - Herb’s MacBook Air/FourthYearProject/perovskite_env/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Documents - Herb’s MacBook Air/FourthYearProject/perovskite_env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Documents - Herb’s MacBook Air/FourthYearProject/perovskite_env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Documents - Herb’s MacBook Air/FourthYearProject/perovskite_env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m     )\n\u001b[0;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Documents - Herb’s MacBook Air/FourthYearProject/perovskite_env/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Documents - Herb’s MacBook Air/FourthYearProject/perovskite_env/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Documents - Herb’s MacBook Air/FourthYearProject/perovskite_env/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Documents - Herb’s MacBook Air/FourthYearProject/perovskite_env/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import warnings\n",
    "import csv\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Load and Filter Data\n",
    "def load_and_filter_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the specified CSV file and filters rows where 'Cell_architecture' is 'nip'.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Filter rows where 'Cell_architecture' is exactly 'nip' (case-insensitive)\n",
    "    data = data[data['Cell_architecture'].str.strip().str.lower() == 'nip']\n",
    "    \n",
    "    # Reset index after filtering\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 2. Define Layer Columns\n",
    "def define_layer_columns():\n",
    "    \"\"\"\n",
    "    Defines the mapping between stack sequence columns and their corresponding layer names.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping column names to layer names.\n",
    "    \"\"\"\n",
    "    layer_columns = {\n",
    "        'Cell_stack_sequence': 'Cell',\n",
    "        'Substrate_stack_sequence': 'Substrate',\n",
    "        'ETL_stack_sequence': 'ETL',\n",
    "        'HTL_stack_sequence': 'HTL',\n",
    "        'Backcontact_stack_sequence': 'Backcontact',\n",
    "        'Add_lay_back_stack_sequence': 'Add_Lay_Back',\n",
    "        'Encapsulation_stack_sequence': 'Encapsulation'\n",
    "    }\n",
    "    return layer_columns\n",
    "\n",
    "# 3. Parse Sequences from Multiple Columns\n",
    "def parse_sequences_from_columns(dataframe, layer_columns):\n",
    "    \"\"\"\n",
    "    Parses material sequences from multiple layer-specific columns and maps materials to their layers.\n",
    "    \n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): The filtered DataFrame.\n",
    "        layer_columns (dict): Dictionary mapping column names to layer names.\n",
    "    \n",
    "    Returns:\n",
    "        list: Tokenized sequences (list of materials).\n",
    "        dict: Mapping of materials to layers with occurrence counts.\n",
    "        list: List of unique layer names.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    material_layer_map = {}  # Material to layers mapping with counts\n",
    "    layer_names = list(layer_columns.values())\n",
    "    \n",
    "    for idx, row in dataframe.iterrows():\n",
    "        sequence = []\n",
    "        for col, layer_name in layer_columns.items():\n",
    "            seq_str = row.get(col, \"\")\n",
    "            if pd.isna(seq_str) or not seq_str.strip():\n",
    "                continue\n",
    "            # Split the sequence into sub-layers if applicable\n",
    "            sub_layers = seq_str.split(' | ')\n",
    "            for sub_layer in sub_layers:\n",
    "                # Split sub-layers into materials\n",
    "                materials = [material.strip() for material in sub_layer.split('; ') if material.strip()]\n",
    "                sequence.extend(materials)\n",
    "                for material in materials:\n",
    "                    if material not in material_layer_map:\n",
    "                        material_layer_map[material] = {}\n",
    "                    if layer_name not in material_layer_map[material]:\n",
    "                        material_layer_map[material][layer_name] = 0\n",
    "                    material_layer_map[material][layer_name] += 1\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return sequences, material_layer_map, layer_names\n",
    "\n",
    "# 4. Train Word2Vec Model\n",
    "def train_word2vec(sequences, vector_size=50, window=5, min_count=1, workers=4, sg=1):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the provided material sequences.\n",
    "    \n",
    "    Parameters:\n",
    "        sequences (list): List of tokenized material sequences.\n",
    "        vector_size (int): Dimensionality of the embeddings.\n",
    "        window (int): Context window size.\n",
    "        min_count (int): Minimum frequency count of materials.\n",
    "        workers (int): Number of worker threads.\n",
    "        sg (int): Training algorithm (1 for skip-gram; otherwise CBOW).\n",
    "    \n",
    "    Returns:\n",
    "        Word2Vec: Trained Word2Vec model.\n",
    "    \"\"\"\n",
    "    model = Word2Vec(\n",
    "        sentences=sequences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 5. Assign Primary Layers to Materials\n",
    "def assign_primary_layers(material_layer_map):\n",
    "    \"\"\"\n",
    "    Assigns each material to its primary layer based on the highest occurrence.\n",
    "    \n",
    "    Parameters:\n",
    "        material_layer_map (dict): Mapping of materials to layers with occurrence counts.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of materials to their primary layer.\n",
    "    \"\"\"\n",
    "    material_primary_layer = {}\n",
    "    for material, layers in material_layer_map.items():\n",
    "        # Assign the material to the layer where it occurs most frequently\n",
    "        primary_layer = max(layers, key=layers.get)\n",
    "        material_primary_layer[material] = primary_layer\n",
    "    return material_primary_layer\n",
    "\n",
    "# 6. Assign Colors to Layers\n",
    "def assign_colors_to_layers(layer_names):\n",
    "    \"\"\"\n",
    "    Assigns distinct colors to each layer using a colormap.\n",
    "    \n",
    "    Parameters:\n",
    "        layer_names (list): List of unique layer names.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of layer names to colors.\n",
    "    \"\"\"\n",
    "    num_layers = len(layer_names)\n",
    "    cmap = cm.get_cmap('tab10', num_layers) if num_layers <= 10 else cm.get_cmap('tab20', num_layers)\n",
    "    \n",
    "    layer_colors = {}\n",
    "    for idx, layer_name in enumerate(layer_names):\n",
    "        layer_colors[layer_name] = cmap(idx)\n",
    "    return layer_colors\n",
    "\n",
    "# 7. Extract Embeddings\n",
    "def extract_embeddings(model, materials):\n",
    "    \"\"\"\n",
    "    Extracts embeddings for each material from the Word2Vec model.\n",
    "    \n",
    "    Parameters:\n",
    "        model (Word2Vec): Trained Word2Vec model.\n",
    "        materials (list): List of materials.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Array of embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = np.array([model.wv[material] for material in materials])\n",
    "    return embeddings\n",
    "\n",
    "# 8. Aggregate Embeddings for Each Sample\n",
    "def aggregate_embeddings(sequences, model, vector_size=50):\n",
    "    \"\"\"\n",
    "    Aggregates material embeddings for each sample by averaging.\n",
    "    \n",
    "    Parameters:\n",
    "        sequences (list): List of tokenized material sequences for each sample.\n",
    "        model (Word2Vec): Trained Word2Vec model.\n",
    "        vector_size (int): Dimensionality of the embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Aggregated feature matrix.\n",
    "    \"\"\"\n",
    "    aggregated_features = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) == 0:\n",
    "            aggregated_features.append(np.zeros(vector_size))\n",
    "            continue\n",
    "        vectors = [model.wv[material] for material in seq if material in model.wv]\n",
    "        if vectors:\n",
    "            aggregated = np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            aggregated = np.zeros(vector_size)\n",
    "        aggregated_features.append(aggregated)\n",
    "    return np.array(aggregated_features)\n",
    "\n",
    "# 9. Plot Embeddings with Color Coding and Labels\n",
    "def plot_embeddings_colored(embeddings_2d, materials, material_primary_layer, layer_colors, title, annotate=True):\n",
    "    \"\"\"\n",
    "    Plots the 2D embeddings with colors based on their primary layers and labels each vector.\n",
    "    \n",
    "    Parameters:\n",
    "        embeddings_2d (np.ndarray): 2D embeddings.\n",
    "        materials (list): List of materials.\n",
    "        material_primary_layer (dict): Mapping of materials to their primary layers.\n",
    "        layer_colors (dict): Mapping of layer names to colors.\n",
    "        title (str): Title of the plot.\n",
    "        annotate (bool): Whether to annotate material names on the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    \n",
    "    # Assign colors to each material based on its primary layer\n",
    "    colors_list = [layer_colors[material_primary_layer[material]] for material in materials]\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=colors_list, alpha=0.7, edgecolors='w', linewidth=0.5)\n",
    "    \n",
    "    # Optionally annotate materials\n",
    "    if annotate:\n",
    "        for i, material in enumerate(materials):\n",
    "            plt.annotate(material, xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                         fontsize=8, alpha=0.75, ha='right', va='bottom')\n",
    "    \n",
    "    # Create legend\n",
    "    legend_handles = []\n",
    "    for layer_name, color in layer_colors.items():\n",
    "        patch = mpatches.Patch(color=color, label=layer_name)\n",
    "        legend_handles.append(patch)\n",
    "    plt.legend(handles=legend_handles, title='Layers', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Dimension 1', fontsize=14)\n",
    "    plt.ylabel('Dimension 2', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 10. Prepare Feature Matrix and Target Vector\n",
    "def prepare_features_targets(aggregated_features, dataframe, target_column='JV_default_PCE'):\n",
    "    \"\"\"\n",
    "    Prepares the feature matrix and target vector for model training.\n",
    "    Handles missing values in the target by removing corresponding samples.\n",
    "    \n",
    "    Parameters:\n",
    "        aggregated_features (np.ndarray): Aggregated feature matrix.\n",
    "        dataframe (pd.DataFrame): Original DataFrame.\n",
    "        target_column (str): Name of the target column.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Feature matrix after handling missing targets.\n",
    "        np.ndarray: Target vector after handling missing targets.\n",
    "    \"\"\"\n",
    "    # Combine features and target into a DataFrame for easier handling\n",
    "    feature_df = pd.DataFrame(aggregated_features)\n",
    "    target_series = dataframe[target_column]\n",
    "    \n",
    "    # Concatenate features and target\n",
    "    combined_df = pd.concat([feature_df, target_series], axis=1)\n",
    "    \n",
    "    # Drop rows where target is NaN\n",
    "    initial_count = combined_df.shape[0]\n",
    "    combined_df = combined_df.dropna(subset=[target_column])\n",
    "    final_count = combined_df.shape[0]\n",
    "    dropped = initial_count - final_count\n",
    "    if dropped > 0:\n",
    "        print(f\"Dropped {dropped} samples due to NaN in target '{target_column}'.\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = combined_df.drop(columns=[target_column]).values\n",
    "    y = combined_df[target_column].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 11. Define Models and Hyperparameters\n",
    "def define_models_hyperparameters():\n",
    "    \"\"\"\n",
    "    Defines the models and their corresponding hyperparameters for optimization.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping model names to their scikit-learn estimator and hyperparameter grid.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'RandomForest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 150],          # Reduced options\n",
    "                'max_depth': [None, 10],             # Fewer options\n",
    "                'min_samples_split': [2, 4],         # Fewer options\n",
    "                'min_samples_leaf': [1, 2]           # Fewer options\n",
    "            }\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'model': GradientBoostingRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 150],          # Reduced options\n",
    "                'learning_rate': [0.05, 0.1],        # Fewer options\n",
    "                'max_depth': [3, 4],                  # Fewer options\n",
    "                'min_samples_split': [2, 4]          # Fewer options\n",
    "            }\n",
    "        }\n",
    "        # You can add more models if time permits\n",
    "    }\n",
    "    return models\n",
    "\n",
    "# 12. Train and Evaluate Models using GridSearchCV\n",
    "def train_evaluate_models_with_grid_search(X, y, models, cv=3):\n",
    "    \"\"\"\n",
    "    Trains and evaluates models using GridSearchCV for hyperparameter tuning.\n",
    "    \n",
    "    Parameters:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (np.ndarray): Target vector.\n",
    "        models (dict): Dictionary of models and their hyperparameters.\n",
    "        cv (int): Number of cross-validation folds.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing model details and performance metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model_name, config in models.items():\n",
    "        estimator = config['model']\n",
    "        param_grid = config['params']\n",
    "        \n",
    "        print(f\"Training {model_name}...\")\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1,  # Utilize all available cores\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X, y)\n",
    "        \n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_\n",
    "        \n",
    "        # To get MAE and MSE, perform cross-validation predictions\n",
    "        # Since GridSearchCV with scoring='r2' does not provide MAE and MSE, we'll compute them manually\n",
    "        # Alternatively, you can use multiple scoring metrics in GridSearchCV\n",
    "        # For simplicity, we'll use cross_val_predict here\n",
    "        \n",
    "        # Re-train the best estimator on the entire dataset\n",
    "        best_estimator = grid_search.best_estimator_\n",
    "        best_estimator.fit(X, y)\n",
    "        y_pred = best_estimator.predict(X)\n",
    "        \n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Parameters': best_params,\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'R2': r2\n",
    "        })\n",
    "        \n",
    "        print(f\"{model_name} Best Params: {best_params} -> MAE: {mae:.4f}, MSE: {mse:.4f}, R2: {r2:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 13. Save Results to CSV\n",
    "def save_results_to_csv(results, filename='model_results.csv'):\n",
    "    \"\"\"\n",
    "    Saves the model training results to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        results (list): List of dictionaries containing model details and performance metrics.\n",
    "        filename (str): Name of the CSV file to save the results.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to save.\")\n",
    "        return\n",
    "    keys = results[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(results)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "# 14. Main Execution Function\n",
    "def main():\n",
    "    # File path to the CSV dataset\n",
    "    file_path = 'perovskite_database_query.csv'\n",
    "    \n",
    "    # 1. Load and filter data\n",
    "    data = load_and_filter_data(file_path)\n",
    "    print(f\"Loaded data with {data.shape[0]} samples.\")\n",
    "    \n",
    "    # 2. Define layer columns and their corresponding layer names\n",
    "    layer_columns = define_layer_columns()\n",
    "    \n",
    "    # 3. Parse sequences from the specified columns\n",
    "    tokenized_sequences, material_layer_map, layer_names = parse_sequences_from_columns(data, layer_columns)\n",
    "    print(f\"Parsed sequences from columns: {list(layer_columns.keys())}\")\n",
    "    \n",
    "    # 4. Train Word2Vec model\n",
    "    model = train_word2vec(tokenized_sequences)\n",
    "    print(\"Trained Word2Vec model.\")\n",
    "    \n",
    "    # 5. Get list of unique materials\n",
    "    materials = list(model.wv.index_to_key)\n",
    "    print(f\"Number of unique materials in specified stack sequences: {len(materials)}\")\n",
    "    \n",
    "    # 6. Assign primary layers to materials\n",
    "    material_primary_layer = assign_primary_layers(material_layer_map)\n",
    "    \n",
    "    # 7. Assign colors to layers\n",
    "    layer_colors = assign_colors_to_layers(layer_names)\n",
    "    \n",
    "    # 8. Extract embeddings\n",
    "    embeddings = extract_embeddings(model, materials)\n",
    "    \n",
    "    # 9. Aggregate embeddings for each sample\n",
    "    aggregated_features = aggregate_embeddings(tokenized_sequences, model, vector_size=model.vector_size)\n",
    "    print(\"Aggregated embeddings for each sample.\")\n",
    "    \n",
    "    # 10. Prepare feature matrix and target vector (Handle Missing Targets)\n",
    "    X, y = prepare_features_targets(aggregated_features, data, target_column='JV_default_PCE')\n",
    "    print(f\"Prepared feature matrix with shape {X.shape} and target vector with shape {y.shape}.\")\n",
    "    \n",
    "    # 11. Define models and hyperparameters\n",
    "    models = define_models_hyperparameters()\n",
    "    \n",
    "    # 12. Train and evaluate models using GridSearchCV\n",
    "    print(\"Starting model training and evaluation...\")\n",
    "    results = train_evaluate_models_with_grid_search(X, y, models, cv=3)\n",
    "    \n",
    "    # 13. Save results to CSV\n",
    "    save_results_to_csv(results, filename='model_results.csv')\n",
    "    \n",
    "    # Optional: Save Word2Vec model for future use\n",
    "    model.save(\"word2vec_model_full_stack_sequence.model\")\n",
    "    print(\"Word2Vec model saved as 'word2vec_model_full_stack_sequence.model'.\")\n",
    "    \n",
    "    # Optional: Save aggregated features and target for future use\n",
    "    np.save('aggregated_features.npy', X)\n",
    "    np.save('target_vector.npy', y)\n",
    "    print(\"Aggregated features and target vector saved.\")\n",
    "    \n",
    "    # # 14. Visualize Embeddings (Optional)\n",
    "    # # PCA Visualization\n",
    "    # pca = PCA(n_components=2, random_state=42)\n",
    "    # reduced_embeddings_pca = pca.fit_transform(embeddings)\n",
    "    # plot_embeddings_colored(\n",
    "    #     embeddings_2d=reduced_embeddings_pca,\n",
    "    #     materials=materials,\n",
    "    #     material_primary_layer=material_primary_layer,\n",
    "    #     layer_colors=layer_colors,\n",
    "    #     title='Material Embeddings with PCA (Colored by Layers)',\n",
    "    #     annotate=False  # Set to True to display annotations\n",
    "    # )\n",
    "    \n",
    "    # # t-SNE Visualization\n",
    "    # tsne = TSNE(n_components=2, perplexity=5, random_state=42, init='random', learning_rate='auto')\n",
    "    # reduced_embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "    # plot_embeddings_colored(\n",
    "    #     embeddings_2d=reduced_embeddings_tsne,\n",
    "    #     materials=materials,\n",
    "    #     material_primary_layer=material_primary_layer,\n",
    "    #     layer_colors=layer_colors,\n",
    "    #     title='Material Embeddings with t-SNE (Colored by Layers)',\n",
    "    #     annotate=False  # Set to True to display annotations\n",
    "    # )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data with 29560 samples.\n",
      "Parsed sequences from columns.\n",
      "Trained Word2Vec model.\n",
      "Aggregated embeddings for each sample.\n",
      "Dropped 624 samples due to NaN in target 'JV_default_PCE'.\n",
      "Prepared feature matrix with shape (28936, 61) and target vector with shape (28936,)\n",
      "Starting model training and evaluation...\n",
      "Training RandomForest...\n",
      "RandomForest Best Params: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 150}\n",
      "MAE: 2.9252, MSE: 14.4245, R2: 0.4897\n",
      "Training GradientBoosting...\n",
      "GradientBoosting Best Params: {'learning_rate': 0.1, 'max_depth': 4, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "MAE: 3.0452, MSE: 15.2214, R2: 0.4615\n",
      "Results saved to model_results.csv\n",
      "Word2Vec model saved as 'word2vec_model_full_stack_sequence.model'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import warnings\n",
    "import csv\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Load and Filter Data\n",
    "def load_and_filter_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the specified CSV file and filters rows where 'Cell_architecture' is 'nip'.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data[data['Cell_architecture'].str.strip().str.lower() == 'nip']\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "# 2. Define Layer Columns\n",
    "def define_layer_columns():\n",
    "    \"\"\"\n",
    "    Defines the mapping between stack sequence columns and their corresponding layer names.\n",
    "    \"\"\"\n",
    "    layer_columns = {\n",
    "        'Cell_stack_sequence': 'Cell',\n",
    "        'Substrate_stack_sequence': 'Substrate',\n",
    "        'ETL_stack_sequence': 'ETL',\n",
    "        'HTL_stack_sequence': 'HTL',\n",
    "        'Backcontact_stack_sequence': 'Backcontact',\n",
    "        'Add_lay_back_stack_sequence': 'Add_Lay_Back',\n",
    "        'Encapsulation_stack_sequence': 'Encapsulation'\n",
    "    }\n",
    "    return layer_columns\n",
    "\n",
    "# 3. Parse Sequences from Multiple Columns\n",
    "def parse_sequences_from_columns(dataframe, layer_columns):\n",
    "    \"\"\"\n",
    "    Parses material sequences from multiple layer-specific columns and maps materials to their layers.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    material_layer_map = {}\n",
    "    layer_names = list(layer_columns.values())\n",
    "    \n",
    "    for idx, row in dataframe.iterrows():\n",
    "        sequence = []\n",
    "        for col, layer_name in layer_columns.items():\n",
    "            seq_str = row.get(col, \"\")\n",
    "            if pd.isna(seq_str) or not seq_str.strip():\n",
    "                continue\n",
    "            sub_layers = seq_str.split(' | ')\n",
    "            for sub_layer in sub_layers:\n",
    "                materials = [material.strip() for material in sub_layer.split('; ') if material.strip()]\n",
    "                sequence.extend(materials)\n",
    "                for material in materials:\n",
    "                    if material not in material_layer_map:\n",
    "                        material_layer_map[material] = {}\n",
    "                    if layer_name not in material_layer_map[material]:\n",
    "                        material_layer_map[material][layer_name] = 0\n",
    "                    material_layer_map[material][layer_name] += 1\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return sequences, material_layer_map, layer_names\n",
    "\n",
    "# 4. Train Word2Vec Model\n",
    "def train_word2vec(sequences, vector_size=50, window=5, min_count=1, workers=4, sg=1):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the provided material sequences.\n",
    "    \"\"\"\n",
    "    model = Word2Vec(\n",
    "        sentences=sequences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def extract_composition_features(dataframe):\n",
    "    \"\"\"\n",
    "    Extracts and processes perovskite composition features.\n",
    "    Ensures all samples have the same number of features.\n",
    "    \"\"\"\n",
    "    composition_columns = [\n",
    "        'Perovskite_composition_a_ions',\n",
    "        'Perovskite_composition_a_ions_coefficients',\n",
    "        'Perovskite_composition_b_ions',\n",
    "        'Perovskite_composition_b_ions_coefficients',\n",
    "        'Perovskite_composition_c_ions',\n",
    "        'Perovskite_composition_c_ions_coefficients'\n",
    "    ]\n",
    "    \n",
    "    # First pass: determine maximum number of coefficients for each ion type\n",
    "    max_coeffs = {'a': 0, 'b': 0, 'c': 0}\n",
    "    \n",
    "    for idx, row in dataframe.iterrows():\n",
    "        for i, ion_type in enumerate(['a', 'b', 'c']):\n",
    "            coeffs_str = str(row[composition_columns[i*2 + 1]]).split(';')\n",
    "            max_coeffs[ion_type] = max(max_coeffs[ion_type], len(coeffs_str))\n",
    "    \n",
    "    total_features = sum(max_coeffs.values())\n",
    "    processed_features = np.zeros((len(dataframe), total_features))\n",
    "    \n",
    "    # Second pass: fill in the features array\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        feature_idx = 0\n",
    "        \n",
    "        # Process each pair of ion and coefficient columns\n",
    "        for i, ion_type in enumerate(['a', 'b', 'c']):\n",
    "            ions = str(row[composition_columns[i*2]]).split(';')\n",
    "            coeffs_str = str(row[composition_columns[i*2 + 1]]).split(';')\n",
    "            \n",
    "            # Convert coefficients to float, using 0.0 for any invalid values\n",
    "            coeffs = []\n",
    "            for coeff in coeffs_str:\n",
    "                try:\n",
    "                    coeffs.append(float(coeff.strip()))\n",
    "                except (ValueError, AttributeError):\n",
    "                    coeffs.append(0.0)\n",
    "            \n",
    "            # Pad with zeros if needed\n",
    "            while len(coeffs) < max_coeffs[ion_type]:\n",
    "                coeffs.append(0.0)\n",
    "            \n",
    "            # Add coefficients to features array\n",
    "            processed_features[idx, feature_idx:feature_idx + len(coeffs)] = coeffs\n",
    "            feature_idx += max_coeffs[ion_type]\n",
    "    \n",
    "    return processed_features\n",
    "\n",
    "# 6. Aggregate Embeddings for Each Sample\n",
    "def aggregate_embeddings(sequences, model, vector_size=50):\n",
    "    \"\"\"\n",
    "    Aggregates material embeddings for each sample by averaging.\n",
    "    \"\"\"\n",
    "    aggregated_features = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) == 0:\n",
    "            aggregated_features.append(np.zeros(vector_size))\n",
    "            continue\n",
    "        vectors = [model.wv[material] for material in seq if material in model.wv]\n",
    "        if vectors:\n",
    "            aggregated = np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            aggregated = np.zeros(vector_size)\n",
    "        aggregated_features.append(aggregated)\n",
    "    return np.array(aggregated_features)\n",
    "\n",
    "# 7. Prepare Features and Targets\n",
    "def prepare_features_targets(aggregated_features, dataframe, target_column='JV_default_PCE'):\n",
    "    \"\"\"\n",
    "    Prepares the feature matrix and target vector for model training.\n",
    "    Now includes both embeddings and composition features.\n",
    "    \"\"\"\n",
    "    # Get composition features\n",
    "    composition_features = extract_composition_features(dataframe)\n",
    "    \n",
    "    # Combine embeddings with composition features\n",
    "    combined_features = np.hstack([aggregated_features, composition_features])\n",
    "    \n",
    "    # Handle missing values in the combined features\n",
    "    combined_features = np.nan_to_num(combined_features, nan=0.0)\n",
    "    \n",
    "    # Combine features and target into a DataFrame for easier handling\n",
    "    feature_df = pd.DataFrame(combined_features)\n",
    "    target_series = dataframe[target_column]\n",
    "    \n",
    "    # Concatenate features and target\n",
    "    combined_df = pd.concat([feature_df, target_series], axis=1)\n",
    "    \n",
    "    # Drop rows where target is NaN\n",
    "    initial_count = combined_df.shape[0]\n",
    "    combined_df = combined_df.dropna(subset=[target_column])\n",
    "    final_count = combined_df.shape[0]\n",
    "    dropped = initial_count - final_count\n",
    "    if dropped > 0:\n",
    "        print(f\"Dropped {dropped} samples due to NaN in target '{target_column}'.\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = combined_df.drop(columns=[target_column]).values\n",
    "    y = combined_df[target_column].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 8. Define Models and Hyperparameters\n",
    "def define_models_hyperparameters():\n",
    "    \"\"\"\n",
    "    Defines the models and their corresponding hyperparameters for optimization.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'RandomForest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 150],\n",
    "                'max_depth': [None, 10],\n",
    "                'min_samples_split': [2, 4],\n",
    "                'min_samples_leaf': [1, 2]\n",
    "            }\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'model': GradientBoostingRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 150],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'max_depth': [3, 4],\n",
    "                'min_samples_split': [2, 4]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return models\n",
    "\n",
    "# 9. Train and Evaluate Models\n",
    "def train_evaluate_models_with_grid_search(X, y, models, cv=3):\n",
    "    \"\"\"\n",
    "    Trains and evaluates models using GridSearchCV for hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model_name, config in models.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=config['model'],\n",
    "            param_grid=config['params'],\n",
    "            cv=cv,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X, y)\n",
    "        \n",
    "        best_params = grid_search.best_params_\n",
    "        best_estimator = grid_search.best_estimator_\n",
    "        best_estimator.fit(X, y)\n",
    "        y_pred = best_estimator.predict(X)\n",
    "        \n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Parameters': best_params,\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'R2': r2\n",
    "        })\n",
    "        \n",
    "        print(f\"{model_name} Best Params: {best_params}\")\n",
    "        print(f\"MAE: {mae:.4f}, MSE: {mse:.4f}, R2: {r2:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 10. Save Results\n",
    "def save_results_to_csv(results, filename='model_results.csv'):\n",
    "    \"\"\"\n",
    "    Saves the model training results to a CSV file.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to save.\")\n",
    "        return\n",
    "    \n",
    "    keys = results[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(results)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "# 11. Main Execution Function\n",
    "def main():\n",
    "    # File path to the CSV dataset\n",
    "    file_path = 'perovskite_database_query.csv'\n",
    "    \n",
    "    # Load and filter data\n",
    "    data = load_and_filter_data(file_path)\n",
    "    print(f\"Loaded data with {data.shape[0]} samples.\")\n",
    "    \n",
    "    # Define layer columns\n",
    "    layer_columns = define_layer_columns()\n",
    "    \n",
    "    # Parse sequences\n",
    "    tokenized_sequences, material_layer_map, layer_names = parse_sequences_from_columns(data, layer_columns)\n",
    "    print(\"Parsed sequences from columns.\")\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    model = train_word2vec(tokenized_sequences)\n",
    "    print(\"Trained Word2Vec model.\")\n",
    "    \n",
    "    # Aggregate embeddings\n",
    "    aggregated_features = aggregate_embeddings(tokenized_sequences, model)\n",
    "    print(\"Aggregated embeddings for each sample.\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X, y = prepare_features_targets(aggregated_features, data)\n",
    "    print(f\"Prepared feature matrix with shape {X.shape} and target vector with shape {y.shape}\")\n",
    "    \n",
    "    # Define models and train\n",
    "    models = define_models_hyperparameters()\n",
    "    print(\"Starting model training and evaluation...\")\n",
    "    results = train_evaluate_models_with_grid_search(X, y, models, cv=3)\n",
    "    \n",
    "    # Save results\n",
    "    save_results_to_csv(results)\n",
    "    \n",
    "    # Save Word2Vec model\n",
    "    model.save(\"word2vec_model_full_stack_sequence.model\")\n",
    "    print(\"Word2Vec model saved as 'word2vec_model_full_stack_sequence.model'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:6: DtypeWarning: Columns (10,22,29,31,32,35,36,40,44,45,46,48,51,54,65,84,89,90,93,98,99,100,105,108,115,118,122,123,125,130,134,138,142,143,144,146,149,152,163,166,167,171,172,173,175,178,181,192,194,225,271,272,273,277,304,315,321,325,330,331,335,336,342,348,369,371,373,374,376,380,384,387,403,405,407,409) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path)\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:119: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Layer Type'] = data.apply(\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:126: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['combined_ions'] = data.apply(\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['combined_coefficients'] = data.apply(\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/1493972200.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['combined_sites'] = data.apply(generate_combined_sites, axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     combined_ions combined_coefficients combined_sites\n",
      "0      [Cs, Sn, I]       [1.0, 1.0, 1.0]      [a, b, c]\n",
      "1  [Cs, Sn, Br, I]  [1.0, 1.0, 0.1, 0.9]   [a, b, c, c]\n",
      "2  [Cs, Sn, Br, I]  [1.0, 1.0, 0.5, 0.5]   [a, b, c, c]\n",
      "3  [Cs, Sn, Br, I]  [1.0, 1.0, 0.9, 0.1]   [a, b, c, c]\n",
      "4     [Cs, Sn, Br]       [1.0, 1.0, 1.0]      [a, b, c]\n",
      "CSV file with layer type information modified and saved as: data_with_layer_type_and_combined.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"perovskite_database_query.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = [\n",
    "    'Cell_stack_sequence', 'Cell_architecture',\n",
    "    'Substrate_stack_sequence', 'Substrate_thickness',\n",
    "    'ETL_stack_sequence', 'ETL_thickness', 'ETL_additives_compounds', 'ETL_additives_concentrations',\n",
    "    'Perovskite_composition_a_ions', 'Perovskite_composition_a_ions_coefficients', \n",
    "    'Perovskite_composition_b_ions', 'Perovskite_composition_b_ions_coefficients',\n",
    "    'Perovskite_composition_c_ions', 'Perovskite_composition_c_ions_coefficients', \n",
    "    'Perovskite_additives_compounds', 'Perovskite_additives_concentrations', 'Perovskite_thickness',\n",
    "    'HTL_stack_sequence', 'HTL_thickness_list', 'HTL_additives_compounds', 'HTL_additives_concentrations',\n",
    "    'Backcontact_stack_sequence', 'Backcontact_thickness', \n",
    "    'Backcontact_additives_compounds', 'Backcontact_additives_concentrations',\n",
    "    'Add_lay_front', 'Add_lay_front_function', 'Add_lay_front_stack_sequence', 'Add_lay_front_thickness_list', \n",
    "    'Add_lay_front_additives_compounds', 'Add_lay_front_additives_concentrations',\n",
    "    'Add_lay_back', 'Add_lay_back_function', 'Add_lay_back_stack_sequence', 'Add_lay_back_thickness_list', \n",
    "    'Add_lay_back_additives_compounds', 'Add_lay_back_additives_concentrations',\n",
    "    'Encapsulation', 'Encapsulation_stack_sequence'\n",
    "]\n",
    "\n",
    "# Filter columns to keep only those that exist in the dataset\n",
    "existing_columns = [col for col in columns_to_keep if col in data.columns]\n",
    "data = data[existing_columns]\n",
    "\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# # Add an index column\n",
    "# data.reset_index(inplace=True)\n",
    "# data.rename(columns={'index': 'Index'}, inplace=True)\n",
    "\n",
    "# # Save the filtered dataset to a new CSV file\n",
    "# output_path = 'filtered_DatabaseMaterials_with_index.csv'\n",
    "# data.to_csv(output_path, index=False)\n",
    "# print(\"Filtered dataset with index saved as\", output_path)\n",
    "\n",
    "# Create a dataframe for ions and their coefficients\n",
    "ion_columns = [\n",
    "    'Perovskite_composition_a_ions', 'Perovskite_composition_a_ions_coefficients', \n",
    "    'Perovskite_composition_b_ions', 'Perovskite_composition_b_ions_coefficients',\n",
    "    'Perovskite_composition_c_ions', 'Perovskite_composition_c_ions_coefficients'\n",
    "]\n",
    "\n",
    "ion_data = data[ion_columns]\n",
    "\n",
    "# # Save the unchanged ion data\n",
    "# output_path = 'ion_data_unchanged.csv'\n",
    "# ion_data.to_csv(output_path, index=False)\n",
    "# print(\"Unchanged ion data saved as\", output_path)\n",
    "\n",
    "# Function to clean molecule names\n",
    "def clean_molecule_name(name):\n",
    "    name = re.sub(r'[^a-zA-Z0-9\\s\\-()]+', ' ', name.strip())\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    elements = [element for element in name.split() if element and not element.replace('.', '', 1).isdigit()]\n",
    "    return elements\n",
    "\n",
    "# Function to clean and convert coefficients to floats\n",
    "def clean_and_convert_coefficient(coefficient):\n",
    "    try:\n",
    "        cleaned_coefficient = re.sub(r'[^0-9.eE-]', '', coefficient.replace(',', '').strip())\n",
    "        return float(cleaned_coefficient) if cleaned_coefficient else 0.0\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "# Function to normalize coefficients\n",
    "def normalize_coefficients(cell):\n",
    "    if pd.notna(cell):\n",
    "        try:\n",
    "            coefficients = [float(x.strip()) for x in re.split(r'[;|]', cell) if x.strip()]\n",
    "            total_sum = sum(coefficients)\n",
    "            return ';'.join(f\"{val / total_sum:.3f}\" for val in coefficients) if total_sum > 0 else cell\n",
    "        except ValueError:\n",
    "            return cell\n",
    "    return cell\n",
    "\n",
    "# Normalize coefficients in each column\n",
    "coefficient_columns = [\n",
    "    'Perovskite_composition_a_ions_coefficients', \n",
    "    'Perovskite_composition_b_ions_coefficients', \n",
    "    'Perovskite_composition_c_ions_coefficients'\n",
    "]\n",
    "\n",
    "for col in coefficient_columns:\n",
    "    data[col] = data[col].apply(normalize_coefficients)\n",
    "\n",
    "# Create a set of unique molecules and add new columns\n",
    "unique_molecules = set()\n",
    "for column_group in ['a', 'b', 'c']:\n",
    "    ions_column = f'Perovskite_composition_{column_group}_ions'\n",
    "    coefficients_column = f'Perovskite_composition_{column_group}_ions_coefficients'\n",
    "    for _, row in data.iterrows():\n",
    "        ions, _ = clean_molecule_name(str(row[ions_column])), [clean_and_convert_coefficient(c) for c in str(row[coefficients_column]).split(';')]\n",
    "        unique_molecules.update(ions)\n",
    "\n",
    "# Create columns for each unique molecule and calculate proportions\n",
    "for molecule in unique_molecules:\n",
    "    data[molecule] = 0.0\n",
    "\n",
    "for index, row in data[coefficient_columns].iterrows():\n",
    "    for column_group in ['a', 'b', 'c']:\n",
    "        ions_column = f'Perovskite_composition_{column_group}_ions'\n",
    "        coefficients_column = f'Perovskite_composition_{column_group}_ions_coefficients'\n",
    "        ions = clean_molecule_name(str(row.get(ions_column, \"\")))\n",
    "        coefficients = [clean_and_convert_coefficient(c) for c in str(row[coefficients_column]).split(';')]\n",
    "        total_coeff = sum(coefficients) if sum(coefficients) != 0 else 1\n",
    "        \n",
    "        for ion, coeff in zip(ions, coefficients):\n",
    "            data.at[index, ion] += coeff / total_coeff\n",
    "\n",
    "\n",
    "# Create a new column 'Layer_Type' to indicate if the row is multilayered or single-layered\n",
    "\n",
    "data['Layer Type'] = data.apply(\n",
    "    lambda row: 'Multi-layered Perovskite' if any('|' in str(row[col]) for col in ion_columns) else 'Single-layered Perovskite',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Add/append columns for 'combined ions' and 'combined coefficients' - vector embedding\n",
    "\n",
    "data['combined_ions'] = data.apply(\n",
    "    lambda row: f\"{row.get('Perovskite_composition_a_ions', '')},{row.get('Perovskite_composition_b_ions', '')},{row.get('Perovskite_composition_c_ions', '')}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "data['combined_coefficients'] = data.apply(\n",
    "    lambda row: f\"{row.get('Perovskite_composition_a_ions_coefficients', '')},{row.get('Perovskite_composition_b_ions_coefficients', '')},{row.get('Perovskite_composition_c_ions_coefficients', '')}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ### make sure all combined ions and combined coefficients are lists\n",
    "\n",
    "import re\n",
    "\n",
    "# Function to convert a string with mixed delimiters to a list\n",
    "def convert_to_list(entry):\n",
    "    if isinstance(entry, str):\n",
    "        # Replace semicolons and pipes with commas for uniformity\n",
    "        entry = re.sub(r'[;|]', ',', entry)\n",
    "        # Split the string by commas and strip spaces around each item\n",
    "        return [item.strip() for item in entry.split(',') if item.strip()]\n",
    "    elif isinstance(entry, list):\n",
    "        return entry  # Already a list, no action needed\n",
    "    else:\n",
    "        return []  # Handle missing or invalid entries\n",
    "\n",
    "# Function to convert string entries to float and handle non-numeric values\n",
    "def safe_convert_to_float(entry):\n",
    "    try:\n",
    "        return float(entry)  # Attempt to convert to float\n",
    "    except ValueError:\n",
    "        return None  # If conversion fails, return None (or handle as needed)\n",
    "\n",
    "\n",
    "### add an indication of perovskite site\n",
    "\n",
    "# Add the 'combined_sites' column\n",
    "def generate_combined_sites(row):\n",
    "    # Split combined ions and coefficients into lists\n",
    "    ions = row['combined_ions']\n",
    "    coefficients = row['combined_coefficients']\n",
    "    \n",
    "    # Assign sites ('a', 'b', 'c') based on the origin of each ion/coefficient\n",
    "    sites = []\n",
    "    site_labels = ['a', 'b', 'c']\n",
    "    for site, ions_col, coeff_col in zip(site_labels, \n",
    "                                         ['Perovskite_composition_a_ions', 'Perovskite_composition_b_ions', 'Perovskite_composition_c_ions'], \n",
    "                                         ['Perovskite_composition_a_ions_coefficients', 'Perovskite_composition_b_ions_coefficients', 'Perovskite_composition_c_ions_coefficients']):\n",
    "        # Count the number of ions and coefficients from this site\n",
    "        num_ions = len(clean_molecule_name(str(row.get(ions_col, \"\"))))\n",
    "        num_coefficients = len(str(row.get(coeff_col, \"\")).split(';'))\n",
    "        \n",
    "        # Append the site label for each ion/coefficient from this site\n",
    "        sites.extend([site] * max(num_ions, num_coefficients))\n",
    "    \n",
    "    return sites\n",
    "\n",
    "# Apply the function to generate the 'combined_sites' column\n",
    "data['combined_sites'] = data.apply(generate_combined_sites, axis=1)\n",
    "\n",
    "def clean_coefficients(coefficients):\n",
    "    \"\"\"\n",
    "    Cleans the coefficients by ensuring all values are numeric.\n",
    "    Invalid or non-numeric values are replaced with 0.0.\n",
    "    If the value is already a float, it is left unchanged.\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for c in coefficients:\n",
    "        if isinstance(c, float):  # If already a float, keep it as is\n",
    "            cleaned.append(c)\n",
    "        elif isinstance(c, str) and c.replace('.', '', 1).isdigit():  # If a valid string representation of a number\n",
    "            cleaned.append(float(c))\n",
    "        else:  # For invalid entries\n",
    "            cleaned.append(0.0)\n",
    "    return cleaned\n",
    "\n",
    "def normalize_coefficients_within_cell(row):\n",
    "    \"\"\"\n",
    "    Normalizes the coefficients for each site ('a', 'b', 'c') within a cell.\n",
    "    Ensures that the sum of coefficients for each site equals 1.\n",
    "    \"\"\"\n",
    "    # Extract ions, coefficients, and sites for the row\n",
    "    ions = row['combined_ions']\n",
    "    coefficients = row['combined_coefficients']\n",
    "    sites = row['combined_sites']\n",
    "    \n",
    "    # Initialize lists for each site\n",
    "    site_a_coeffs = []\n",
    "    site_b_coeffs = []\n",
    "    site_c_coeffs = []\n",
    "    \n",
    "    # Separate the coefficients by their sites\n",
    "    for coeff, site in zip(coefficients, sites):\n",
    "        try:\n",
    "            coeff = float(coeff)  # Ensure coefficients are numeric\n",
    "        except ValueError:\n",
    "            coeff = 0.0  # Default to 0.0 if invalid\n",
    "        if site == 'a':\n",
    "            site_a_coeffs.append(coeff)\n",
    "        elif site == 'b':\n",
    "            site_b_coeffs.append(coeff)\n",
    "        elif site == 'c':\n",
    "            site_c_coeffs.append(coeff)\n",
    "    \n",
    "    # Normalize the coefficients for each site if their sum is not zero\n",
    "    def normalize(site_coeffs):\n",
    "        total = sum(site_coeffs)\n",
    "        return [coeff / total for coeff in site_coeffs] if total > 0 else site_coeffs\n",
    "    \n",
    "    site_a_coeffs = normalize(site_a_coeffs)\n",
    "    site_b_coeffs = normalize(site_b_coeffs)\n",
    "    site_c_coeffs = normalize(site_c_coeffs)\n",
    "    \n",
    "    # Combine all coefficients back into a single list\n",
    "    normalized_coeffs = site_a_coeffs + site_b_coeffs + site_c_coeffs\n",
    "\n",
    "    return normalized_coeffs\n",
    "\n",
    "\n",
    "\n",
    "# Apply the function to both columns\n",
    "data['combined_ions'] = data['combined_ions'].apply(convert_to_list)\n",
    "data['combined_coefficients'] = data['combined_coefficients'].apply(\n",
    "    lambda x: [safe_convert_to_float(item) for item in convert_to_list(x)]  # Convert to float for coefficients, handle errors\n",
    ")\n",
    "\n",
    "# Step 1: Clean the coefficients column\n",
    "data['combined_coefficients'] = data['combined_coefficients'].apply(clean_coefficients)\n",
    "\n",
    "# Step 2: Normalize coefficients within each cell\n",
    "data['combined_coefficients'] = data.apply(normalize_coefficients_within_cell, axis=1)\n",
    "\n",
    "# Verify the transformation\n",
    "print(data[['combined_ions', 'combined_coefficients', 'combined_sites']].head())\n",
    "\n",
    "\n",
    "# Drop the original ion columns\n",
    "data = data.drop(columns=ion_columns, errors='ignore')\n",
    "\n",
    "# Save the modified DataFrame with the 'Layer Type' and combined columns\n",
    "output_file_path = 'data_with_layer_type_and_combined.csv'\n",
    "data.to_csv(output_file_path, index=False)\n",
    "print(\"CSV file with layer type information modified and saved as:\", output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:6: DtypeWarning: Columns (10,22,29,31,32,35,36,40,44,45,46,48,51,54,65,84,89,90,93,98,99,100,105,108,115,118,122,123,125,130,134,138,142,143,144,146,149,152,163,166,167,171,172,173,175,178,181,192,194,225,271,272,273,277,304,315,321,325,330,331,335,336,342,348,369,371,373,374,376,380,384,387,403,405,407,409) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path)\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[molecule] = 0.0\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Layer Type'] = data.apply(\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['combined_ions'] = data.apply(\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['combined_coefficients'] = data.apply(\n",
      "/var/folders/j3/pj3kpjws5gj32d8zrny204340000gn/T/ipykernel_40092/3504305608.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['combined_sites'] = data.apply(generate_combined_sites, axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file with layer type information modified and saved as: data_with_layer_type_and_combined.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"perovskite_database_query.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = [\n",
    "    'Cell_stack_sequence', 'Cell_architecture',\n",
    "    'Substrate_stack_sequence', 'Substrate_thickness',\n",
    "    'ETL_stack_sequence', 'ETL_thickness', 'ETL_additives_compounds', 'ETL_additives_concentrations',\n",
    "    'Perovskite_composition_a_ions', 'Perovskite_composition_a_ions_coefficients', \n",
    "    'Perovskite_composition_b_ions', 'Perovskite_composition_b_ions_coefficients',\n",
    "    'Perovskite_composition_c_ions', 'Perovskite_composition_c_ions_coefficients', \n",
    "    'Perovskite_additives_compounds', 'Perovskite_additives_concentrations', 'Perovskite_thickness',\n",
    "    'HTL_stack_sequence', 'HTL_thickness_list', 'HTL_additives_compounds', 'HTL_additives_concentrations',\n",
    "    'Backcontact_stack_sequence', 'Backcontact_thickness', \n",
    "    'Backcontact_additives_compounds', 'Backcontact_additives_concentrations',\n",
    "    'Add_lay_front', 'Add_lay_front_function', 'Add_lay_front_stack_sequence', 'Add_lay_front_thickness_list', \n",
    "    'Add_lay_front_additives_compounds', 'Add_lay_front_additives_concentrations',\n",
    "    'Add_lay_back', 'Add_lay_back_function', 'Add_lay_back_stack_sequence', 'Add_lay_back_thickness_list', \n",
    "    'Add_lay_back_additives_compounds', 'Add_lay_back_additives_concentrations',\n",
    "    'Encapsulation', 'Encapsulation_stack_sequence',\n",
    "    'JV_default_PCE'  # Added the target column\n",
    "]\n",
    "\n",
    "# Filter columns to keep only those that exist in the dataset\n",
    "existing_columns = [col for col in columns_to_keep if col in data.columns]\n",
    "data = data[existing_columns]\n",
    "\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Function to clean molecule names\n",
    "def clean_molecule_name(name):\n",
    "    name = re.sub(r'[^a-zA-Z0-9\\s\\-()]+', ' ', name.strip())\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    elements = [element for element in name.split() if element and not element.replace('.', '', 1).isdigit()]\n",
    "    return elements\n",
    "\n",
    "# Function to clean and convert coefficients to floats\n",
    "def clean_and_convert_coefficient(coefficient):\n",
    "    try:\n",
    "        cleaned_coefficient = re.sub(r'[^0-9.eE-]', '', coefficient.replace(',', '').strip())\n",
    "        return float(cleaned_coefficient) if cleaned_coefficient else 0.0\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "# Function to normalize coefficients\n",
    "def normalize_coefficients(cell):\n",
    "    if pd.notna(cell):\n",
    "        try:\n",
    "            coefficients = [float(x.strip()) for x in re.split(r'[;|]', cell) if x.strip()]\n",
    "            total_sum = sum(coefficients)\n",
    "            return ';'.join(f\"{val / total_sum:.3f}\" for val in coefficients) if total_sum > 0 else cell\n",
    "        except ValueError:\n",
    "            return cell\n",
    "    return cell\n",
    "\n",
    "# Normalize coefficients in each column\n",
    "coefficient_columns = [\n",
    "    'Perovskite_composition_a_ions_coefficients', \n",
    "    'Perovskite_composition_b_ions_coefficients', \n",
    "    'Perovskite_composition_c_ions_coefficients'\n",
    "]\n",
    "\n",
    "for col in coefficient_columns:\n",
    "    data[col] = data[col].apply(normalize_coefficients)\n",
    "\n",
    "# Create a set of unique molecules and add new columns\n",
    "unique_molecules = set()\n",
    "for index, row in data.iterrows():\n",
    "    for column_group in ['a', 'b', 'c']:\n",
    "        ions_column = f'Perovskite_composition_{column_group}_ions'\n",
    "        coefficients_column = f'Perovskite_composition_{column_group}_ions_coefficients'\n",
    "        ions = clean_molecule_name(str(row.get(ions_column, \"\")))\n",
    "        coefficients = [clean_and_convert_coefficient(c) for c in str(row.get(coefficients_column, \"\")).split(';')]\n",
    "        unique_molecules.update(ions)\n",
    "\n",
    "# Create columns for each unique molecule and initialize to zero\n",
    "for molecule in unique_molecules:\n",
    "    data[molecule] = 0.0\n",
    "\n",
    "# Populate the molecule columns with coefficients\n",
    "for index, row in data.iterrows():\n",
    "    for column_group in ['a', 'b', 'c']:\n",
    "        ions_column = f'Perovskite_composition_{column_group}_ions'\n",
    "        coefficients_column = f'Perovskite_composition_{column_group}_ions_coefficients'\n",
    "        ions = clean_molecule_name(str(row.get(ions_column, \"\")))\n",
    "        coefficients = [clean_and_convert_coefficient(c) for c in str(row.get(coefficients_column, \"\")).split(';')]\n",
    "        total_coeff = sum(coefficients) if sum(coefficients) != 0 else 1\n",
    "        for ion, coeff in zip(ions, coefficients):\n",
    "            data.at[index, ion] += coeff / total_coeff\n",
    "\n",
    "# Create a new column 'Layer Type' to indicate if the row is multilayered or single-layered\n",
    "ion_columns = [\n",
    "    'Perovskite_composition_a_ions', 'Perovskite_composition_a_ions_coefficients', \n",
    "    'Perovskite_composition_b_ions', 'Perovskite_composition_b_ions_coefficients',\n",
    "    'Perovskite_composition_c_ions', 'Perovskite_composition_c_ions_coefficients'\n",
    "]\n",
    "\n",
    "data['Layer Type'] = data.apply(\n",
    "    lambda row: 'Multi-layered Perovskite' if any('|' in str(row[col]) for col in ion_columns) else 'Single-layered Perovskite',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Add combined ions and coefficients columns\n",
    "data['combined_ions'] = data.apply(\n",
    "    lambda row: f\"{row.get('Perovskite_composition_a_ions', '')},{row.get('Perovskite_composition_b_ions', '')},{row.get('Perovskite_composition_c_ions', '')}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "data['combined_coefficients'] = data.apply(\n",
    "    lambda row: f\"{row.get('Perovskite_composition_a_ions_coefficients', '')},{row.get('Perovskite_composition_b_ions_coefficients', '')},{row.get('Perovskite_composition_c_ions_coefficients', '')}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Convert combined columns to lists and clean coefficients\n",
    "import re\n",
    "\n",
    "def convert_to_list(entry):\n",
    "    if isinstance(entry, str):\n",
    "        entry = re.sub(r'[;|]', ',', entry)\n",
    "        return [item.strip() for item in entry.split(',') if item.strip()]\n",
    "    elif isinstance(entry, list):\n",
    "        return entry\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def safe_convert_to_float(entry):\n",
    "    try:\n",
    "        return float(entry)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "def generate_combined_sites(row):\n",
    "    ions = row['combined_ions']\n",
    "    coefficients = row['combined_coefficients']\n",
    "    sites = []\n",
    "    site_labels = ['a', 'b', 'c']\n",
    "    for site, ions_col, coeff_col in zip(site_labels, \n",
    "                                         ['Perovskite_composition_a_ions', 'Perovskite_composition_b_ions', 'Perovskite_composition_c_ions'], \n",
    "                                         ['Perovskite_composition_a_ions_coefficients', 'Perovskite_composition_b_ions_coefficients', 'Perovskite_composition_c_ions_coefficients']):\n",
    "        num_ions = len(clean_molecule_name(str(row.get(ions_col, \"\"))))\n",
    "        num_coefficients = len(str(row.get(coeff_col, \"\")).split(';'))\n",
    "        sites.extend([site] * max(num_ions, num_coefficients))\n",
    "    return sites\n",
    "\n",
    "data['combined_ions'] = data['combined_ions'].apply(convert_to_list)\n",
    "data['combined_coefficients'] = data['combined_coefficients'].apply(\n",
    "    lambda x: [safe_convert_to_float(item) for item in convert_to_list(x)]\n",
    ")\n",
    "data['combined_sites'] = data.apply(generate_combined_sites, axis=1)\n",
    "\n",
    "def clean_coefficients(coefficients):\n",
    "    cleaned = []\n",
    "    for c in coefficients:\n",
    "        if isinstance(c, float):\n",
    "            cleaned.append(c)\n",
    "        elif isinstance(c, str) and c.replace('.', '', 1).isdigit():\n",
    "            cleaned.append(float(c))\n",
    "        else:\n",
    "            cleaned.append(0.0)\n",
    "    return cleaned\n",
    "\n",
    "def normalize_coefficients_within_cell(row):\n",
    "    ions = row['combined_ions']\n",
    "    coefficients = row['combined_coefficients']\n",
    "    sites = row['combined_sites']\n",
    "    site_a_coeffs = []\n",
    "    site_b_coeffs = []\n",
    "    site_c_coeffs = []\n",
    "    for coeff, site in zip(coefficients, sites):\n",
    "        try:\n",
    "            coeff = float(coeff)\n",
    "        except ValueError:\n",
    "            coeff = 0.0\n",
    "        if site == 'a':\n",
    "            site_a_coeffs.append(coeff)\n",
    "        elif site == 'b':\n",
    "            site_b_coeffs.append(coeff)\n",
    "        elif site == 'c':\n",
    "            site_c_coeffs.append(coeff)\n",
    "    def normalize(site_coeffs):\n",
    "        total = sum(site_coeffs)\n",
    "        return [coeff / total if total > 0 else 0.0 for coeff in site_coeffs]\n",
    "    site_a_coeffs = normalize(site_a_coeffs)\n",
    "    site_b_coeffs = normalize(site_b_coeffs)\n",
    "    site_c_coeffs = normalize(site_c_coeffs)\n",
    "    normalized_coeffs = site_a_coeffs + site_b_coeffs + site_c_coeffs\n",
    "    return normalized_coeffs\n",
    "\n",
    "data['combined_coefficients'] = data['combined_coefficients'].apply(clean_coefficients)\n",
    "data['combined_coefficients'] = data.apply(normalize_coefficients_within_cell, axis=1)\n",
    "\n",
    "# Drop the original ion columns\n",
    "data = data.drop(columns=ion_columns, errors='ignore')\n",
    "\n",
    "# Save the modified DataFrame\n",
    "output_file_path = 'data_with_layer_type_and_combined.csv'\n",
    "data.to_csv(output_file_path, index=False)\n",
    "print(\"CSV file with layer type information modified and saved as:\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2447236970.py, line 124)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 124\u001b[0;36m\u001b[0m\n\u001b[0;31m    existing_columns =\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import csv\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Load and Filter Data\n",
    "def load_and_filter_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data[data['Cell_architecture'].str.strip().str.lower() == 'nip']\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "# 2. Define Layer Columns\n",
    "def define_layer_columns():\n",
    "    layer_columns = {\n",
    "        'Cell_stack_sequence': 'Cell',\n",
    "        'Substrate_stack_sequence': 'Substrate',\n",
    "        'ETL_stack_sequence': 'ETL',\n",
    "        'HTL_stack_sequence': 'HTL',\n",
    "        'Backcontact_stack_sequence': 'Backcontact',\n",
    "        'Add_lay_back_stack_sequence': 'Add_Lay_Back',\n",
    "        'Encapsulation_stack_sequence': 'Encapsulation'\n",
    "    }\n",
    "    return layer_columns\n",
    "\n",
    "# 3. Parse Sequences from Multiple Columns\n",
    "def parse_sequences_from_columns(dataframe, layer_columns):\n",
    "    sequences = []\n",
    "    material_layer_map = {}\n",
    "    layer_names = list(layer_columns.values())\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        sequence = []\n",
    "        for col, layer_name in layer_columns.items():\n",
    "            seq_str = row.get(col, \"\")\n",
    "            if pd.isna(seq_str) or not seq_str.strip():\n",
    "                continue\n",
    "            sub_layers = seq_str.split(' | ')\n",
    "            for sub_layer in sub_layers:\n",
    "                materials = [material.strip() for material in sub_layer.split('; ') if material.strip()]\n",
    "                sequence.extend(materials)\n",
    "                for material in materials:\n",
    "                    if material not in material_layer_map:\n",
    "                        material_layer_map[material] = {}\n",
    "                    if layer_name not in material_layer_map[material]:\n",
    "                        material_layer_map[material][layer_name] = 0\n",
    "                    material_layer_map[material][layer_name] += 1\n",
    "        sequences.append(sequence)\n",
    "    return sequences, material_layer_map, layer_names\n",
    "\n",
    "# 4. Train Word2Vec Model\n",
    "def train_word2vec(sequences, vector_size=50, window=5, min_count=1, workers=4, sg=1):\n",
    "    model = Word2Vec(\n",
    "        sentences=sequences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 5. Extract Composition Features\n",
    "def extract_composition_features(dataframe):\n",
    "    composition_columns = [\n",
    "        'Perovskite_composition_a_ions',\n",
    "        'Perovskite_composition_a_ions_coefficients',\n",
    "        'Perovskite_composition_b_ions',\n",
    "        'Perovskite_composition_b_ions_coefficients',\n",
    "        'Perovskite_composition_c_ions',\n",
    "        'Perovskite_composition_c_ions_coefficients'\n",
    "    ]\n",
    "    max_coeffs = {'a': 0, 'b': 0, 'c': 0}\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        for i, ion_type in enumerate(['a', 'b', 'c']):\n",
    "            coeffs_str = str(row.get(composition_columns[i*2 + 1], \"\")).split(';')\n",
    "            max_coeffs[ion_type] = max(max_coeffs[ion_type], len(coeffs_str))\n",
    "    total_features = sum(max_coeffs.values())\n",
    "    processed_features = np.zeros((len(dataframe), total_features))\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        feature_idx = 0\n",
    "        for i, ion_type in enumerate(['a', 'b', 'c']):\n",
    "            ions = str(row.get(composition_columns[i*2], \"\")).split(';')\n",
    "            coeffs_str = str(row.get(composition_columns[i*2 + 1], \"\")).split(';')\n",
    "            coeffs = []\n",
    "            for coeff in coeffs_str:\n",
    "                try:\n",
    "                    coeffs.append(float(coeff.strip()))\n",
    "                except (ValueError, AttributeError):\n",
    "                    coeffs.append(0.0)\n",
    "            while len(coeffs) < max_coeffs[ion_type]:\n",
    "                coeffs.append(0.0)\n",
    "            processed_features[idx, feature_idx:feature_idx + len(coeffs)] = coeffs\n",
    "            feature_idx += max_coeffs[ion_type]\n",
    "    return processed_features\n",
    "\n",
    "# 6. Aggregate Embeddings for Each Sample\n",
    "def aggregate_embeddings(sequences, model, vector_size=50):\n",
    "    aggregated_features = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) == 0:\n",
    "            aggregated_features.append(np.zeros(vector_size))\n",
    "            continue\n",
    "        vectors = [model.wv[material] for material in seq if material in model.wv]\n",
    "        if vectors:\n",
    "            aggregated = np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            aggregated = np.zeros(vector_size)\n",
    "        aggregated_features.append(aggregated)\n",
    "    return np.array(aggregated_features)\n",
    "\n",
    "# 7. Prepare Features and Targets\n",
    "def prepare_features_targets(aggregated_features, dataframe, target_column='JV_default_PCE', additional_columns=None):\n",
    "    composition_features = extract_composition_features(dataframe)\n",
    "    combined_features = np.hstack([aggregated_features, composition_features])\n",
    "    combined_features = np.nan_to_num(combined_features, nan=0.0)\n",
    "    if additional_columns:\n",
    "        existing_columns =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data with 29560 samples.\n",
      "Parsed sequences from columns.\n",
      "Trained Word2Vec model.\n",
      "Aggregated embeddings for each sample.\n",
      "The following specified molecule columns are not in the dataframe and will be skipped: ['DAP', 'PEI', 'ThFA', 'PPA', 'PDMA', 'FEA', 'PyrEA', 'PBA', 'PTA', 'CPEA', 'TEA', 'mF1PEA', 'BI', 'oF1PEA', '1', 'iPA', 'ODA', 'Ada', 'N-EtPy', 'MIC1', 'AVA', 'CH3)3S', 'BIM', '4AMP', 'A43', 'CH3)3S', 'PPEA', 'F5PEA', 'C4H9N2H6', '5-AVAI', 'NH4', '4AMPY', '3-Pr(NH3)2', 'ALA', 'MTEA', 'Cl-PEA', 'iso-BA', 'DPA', 'BYA', 'HTAB', 'CHMA', 'F3EA', '6-ACA', 'ImEA', 'HEA', 'APMim', 'C8H17NH3', 'Br-PEA', 'MIC2', 'PGA', '5-AVA', 'BEA', '3AMP', 'F-PEA', 'C6H4NH2', 'CH3ND3', '4FPEA', 'DAT', 'Anyl', 'TBA', '4ApyH', 'pF1PEA', 'TMA', '3AMPY', 'IEA', 'NMA', 'pFPEA', 'EU-pyP', 'PyEA', 'BzDA', 'Ace', 'oFPEA', 'f-PEA', 'C4H9NH3', 'CIEA', 'mFPEA', 'HdA', 'GABA', 'EPA', 'OdA', 'THM', 'BF4', 'FPEA', 'MIC3', 'ThMA', 'BZA', 'H-PEA', 'TFEA', 'n-C3H7NH3', 'BdA', 'C6H13NH3', 'HAD']\n",
      "Dropped 624 samples due to NaN in target 'JV_default_PCE'.\n",
      "Prepared feature matrix with shape (28936, 137) and target vector with shape (28936,)\n",
      "Starting model training and evaluation...\n",
      "Training RandomForest...\n",
      "RandomForest Best Params: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "MAE: 2.8679, MSE: 13.9714, R2: 0.5057\n",
      "Training GradientBoosting...\n",
      "GradientBoosting Best Params: {'learning_rate': 0.1, 'max_depth': 4, 'min_samples_split': 4, 'n_estimators': 100}\n",
      "MAE: 3.0121, MSE: 14.8989, R2: 0.4729\n",
      "Results saved to model_results.csv\n",
      "Word2Vec model saved as 'word2vec_model_full_stack_sequence.model'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "import warnings\n",
    "import csv\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Function to parse molecule columns from the given string\n",
    "def parse_column_names(s):\n",
    "    \"\"\"\n",
    "    Parses column names from a string, removing parentheses and whitespace.\n",
    "    \"\"\"\n",
    "    # Remove any whitespace and trailing commas\n",
    "    s = s.strip().rstrip(',')\n",
    "    # Split on commas\n",
    "    items = s.split(',')\n",
    "    # Clean each item\n",
    "    columns = []\n",
    "    for item in items:\n",
    "        item = item.strip()\n",
    "        # Remove enclosing parentheses, if any\n",
    "        item = item.strip('()')\n",
    "        if item:  # Ensure the item is not empty\n",
    "            columns.append(item)\n",
    "    return columns\n",
    "\n",
    "# 1. Load and Filter Data\n",
    "def load_and_filter_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the specified CSV file and filters rows where 'Cell_architecture' is 'nip'.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data[data['Cell_architecture'].str.strip().str.lower() == 'nip']\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "# 2. Define Layer Columns\n",
    "def define_layer_columns():\n",
    "    \"\"\"\n",
    "    Defines the mapping between stack sequence columns and their corresponding layer names.\n",
    "    \"\"\"\n",
    "    layer_columns = {\n",
    "        'Cell_stack_sequence': 'Cell',\n",
    "        'Substrate_stack_sequence': 'Substrate',\n",
    "        'ETL_stack_sequence': 'ETL',\n",
    "        'HTL_stack_sequence': 'HTL',\n",
    "        'Backcontact_stack_sequence': 'Backcontact',\n",
    "        'Add_lay_back_stack_sequence': 'Add_Lay_Back',\n",
    "        'Encapsulation_stack_sequence': 'Encapsulation'\n",
    "    }\n",
    "    return layer_columns\n",
    "\n",
    "# 3. Parse Sequences from Multiple Columns\n",
    "def parse_sequences_from_columns(dataframe, layer_columns):\n",
    "    \"\"\"\n",
    "    Parses material sequences from multiple layer-specific columns and maps materials to their layers.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    material_layer_map = {}\n",
    "    layer_names = list(layer_columns.values())\n",
    "    \n",
    "    for idx, row in dataframe.iterrows():\n",
    "        sequence = []\n",
    "        for col, layer_name in layer_columns.items():\n",
    "            seq_str = row.get(col, \"\")\n",
    "            if pd.isna(seq_str) or not seq_str.strip():\n",
    "                continue\n",
    "            sub_layers = seq_str.split(' | ')\n",
    "            for sub_layer in sub_layers:\n",
    "                materials = [material.strip() for material in sub_layer.split('; ') if material.strip()]\n",
    "                sequence.extend(materials)\n",
    "                for material in materials:\n",
    "                    if material not in material_layer_map:\n",
    "                        material_layer_map[material] = {}\n",
    "                    if layer_name not in material_layer_map[material]:\n",
    "                        material_layer_map[material][layer_name] = 0\n",
    "                    material_layer_map[material][layer_name] += 1\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return sequences, material_layer_map, layer_names\n",
    "\n",
    "# 4. Train Word2Vec Model\n",
    "def train_word2vec(sequences, vector_size=50, window=5, min_count=1, workers=4, sg=1):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the provided material sequences.\n",
    "    \"\"\"\n",
    "    model = Word2Vec(\n",
    "        sentences=sequences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 5. Prepare Features and Targets\n",
    "def prepare_features_targets(aggregated_features, dataframe, target_column='JV_default_PCE'):\n",
    "    \"\"\"\n",
    "    Prepares the feature matrix and target vector for model training.\n",
    "    Now includes embeddings and specified molecule columns.\n",
    "    \"\"\"\n",
    "    # Get the specified molecule columns\n",
    "    molecule_columns_str = '(DAP),(PEI),(ThFA),Sn,S,Tb,Sm,TN,(PPA),(PDMA),(FEA),(PyrEA),OA,(PBA),(PTA),(CPEA),(TEA),(mF1PEA),FA,(BI),IM,(oF1PEA),(1,(PA),(iPA),Mg,Y,PR,(PF6),(ODA),F,BU,(Ada),Ca,NEA,(SCN),(N-EtPy),HA,(MIC1),Br,(AVA),((CH3)3S),(BIM),Mn,MA,(4AMP),(A43),(CH3)3S,(PPEA),(F5PEA),(C4H9N2H6),(5-AVAI),Sr,(DMA),(NEA),CA,Al,(NH4),(4AMPY),PN,3-Pr(NH3)2),Sb,(PDA),(ALA),Nb,Te,TA,(MTEA),(Cl-PEA),(iso-BA),PF6,(DPA),(BYA),DA,Bi,(HTAB),AN,NMABr,(CHMA),(F3EA),In,(6-ACA),GU,(ImEA),(HEA),IA,Aa,(APMim),(C8H17NH3),(Br-PEA),PMA,(MIC2),(PGA),I,(5-AVA),(PEA),K,(BEA),(PMA),Eu,Cl,(3AMP),(F-PEA),PEA,(C6H4NH2),(CH3ND3),(4FPEA),(DAT),(Anyl),(TBA),(4ApyH),Ba,(pF1PEA),(TMA),Rb,(3AMPY),(IEA),nan,(NMA),Ni,(pFPEA),BE,(EU-pyP),(PyEA),(BzDA),Co,(Ace),Hg,Pb,EDA,(oFPEA),Bn,(f-PEA),(C4H9NH3),(CIEA),(mFPEA),BA,DI,(HdA),PDA,(GABA),Cu,PA,DMA,Na,(EPA),(OdA),(THM),Ge,HDA,(BF4),(FPEA),(MIC3),GA,(ThMA),Cs,(BZA),Au,(H-PEA),Ag,SCN,(TFEA),EA,FPEAI,Fe,(n-C3H7NH3),(BdA),(EDA),BDA,Cr,Pt,Ti,(C6H13NH3),(HAD),Li,(BDA),O,La,Zn,'\n",
    "    \n",
    "    # Parse the molecule columns\n",
    "    molecule_columns = parse_column_names(molecule_columns_str)\n",
    "    \n",
    "    # Keep only columns that exist in the dataframe\n",
    "    existing_molecule_columns = [col for col in molecule_columns if col in dataframe.columns]\n",
    "    missing_columns = [col for col in molecule_columns if col not in dataframe.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"The following specified molecule columns are not in the dataframe and will be skipped: {missing_columns}\")\n",
    "    \n",
    "    # Extract the specified molecule columns from the dataframe\n",
    "    molecule_features = dataframe[existing_molecule_columns]\n",
    "    \n",
    "    # Handle missing values in molecule features\n",
    "    molecule_features = molecule_features.fillna(0.0)\n",
    "    \n",
    "    # Combine embeddings and molecule features\n",
    "    combined_features = np.hstack([aggregated_features, molecule_features.values])\n",
    "    \n",
    "    # Handle missing values in the combined features\n",
    "    combined_features = np.nan_to_num(combined_features, nan=0.0)\n",
    "    \n",
    "    # Combine features and target into a DataFrame for easier handling\n",
    "    feature_df = pd.DataFrame(combined_features)\n",
    "    target_series = dataframe[target_column]\n",
    "    \n",
    "    # Concatenate features and target\n",
    "    combined_df = pd.concat([feature_df, target_series], axis=1)\n",
    "    \n",
    "    # Drop rows where target is NaN\n",
    "    initial_count = combined_df.shape[0]\n",
    "    combined_df = combined_df.dropna(subset=[target_column])\n",
    "    final_count = combined_df.shape[0]\n",
    "    dropped = initial_count - final_count\n",
    "    if dropped > 0:\n",
    "        print(f\"Dropped {dropped} samples due to NaN in target '{target_column}'.\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = combined_df.drop(columns=[target_column]).values\n",
    "    y = combined_df[target_column].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 6. Aggregate Embeddings for Each Sample\n",
    "def aggregate_embeddings(sequences, model, vector_size=50):\n",
    "    \"\"\"\n",
    "    Aggregates material embeddings for each sample by averaging.\n",
    "    \"\"\"\n",
    "    aggregated_features = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) == 0:\n",
    "            aggregated_features.append(np.zeros(vector_size))\n",
    "            continue\n",
    "        vectors = [model.wv[material] for material in seq if material in model.wv]\n",
    "        if vectors:\n",
    "            aggregated = np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            aggregated = np.zeros(vector_size)\n",
    "        aggregated_features.append(aggregated)\n",
    "    return np.array(aggregated_features)\n",
    "\n",
    "# 7. Define Models and Hyperparameters\n",
    "def define_models_hyperparameters():\n",
    "    \"\"\"\n",
    "    Defines the models and their corresponding hyperparameters for optimization.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'RandomForest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 150],\n",
    "                'max_depth': [None, 10],\n",
    "                'min_samples_split': [2, 4],\n",
    "                'min_samples_leaf': [1, 2]\n",
    "            }\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'model': GradientBoostingRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 150],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'max_depth': [3, 4],\n",
    "                'min_samples_split': [2, 4]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return models\n",
    "\n",
    "# 8. Train and Evaluate Models\n",
    "def train_evaluate_models_with_grid_search(X, y, models, cv=3):\n",
    "    \"\"\"\n",
    "    Trains and evaluates models using GridSearchCV for hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model_name, config in models.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=config['model'],\n",
    "            param_grid=config['params'],\n",
    "            cv=cv,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X, y)\n",
    "        \n",
    "        best_params = grid_search.best_params_\n",
    "        best_estimator = grid_search.best_estimator_\n",
    "        best_estimator.fit(X, y)\n",
    "        y_pred = best_estimator.predict(X)\n",
    "        \n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Parameters': best_params,\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'R2': r2\n",
    "        })\n",
    "        \n",
    "        print(f\"{model_name} Best Params: {best_params}\")\n",
    "        print(f\"MAE: {mae:.4f}, MSE: {mse:.4f}, R2: {r2:.4f}\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "# 9. Save Results\n",
    "def save_results_to_csv(results, filename='model_results.csv'):\n",
    "    \"\"\"\n",
    "    Saves the model training results to a CSV file.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to save.\")\n",
    "        return\n",
    "    \n",
    "    keys = results[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(results)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "# 10. Main Execution Function\n",
    "def main():\n",
    "    # File path to the CSV dataset\n",
    "    file_path = 'data_with_layer_type_and_combined.csv'\n",
    "    \n",
    "    # Load and filter data\n",
    "    data = load_and_filter_data(file_path)\n",
    "    print(f\"Loaded data with {data.shape[0]} samples.\")\n",
    "    \n",
    "    # Define layer columns\n",
    "    layer_columns = define_layer_columns()\n",
    "    \n",
    "    # Parse sequences\n",
    "    tokenized_sequences, material_layer_map, layer_names = parse_sequences_from_columns(data, layer_columns)\n",
    "    print(\"Parsed sequences from columns.\")\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    model = train_word2vec(tokenized_sequences)\n",
    "    print(\"Trained Word2Vec model.\")\n",
    "    \n",
    "    # Aggregate embeddings\n",
    "    aggregated_features = aggregate_embeddings(tokenized_sequences, model)\n",
    "    print(\"Aggregated embeddings for each sample.\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X, y = prepare_features_targets(aggregated_features, data)\n",
    "    print(f\"Prepared feature matrix with shape {X.shape} and target vector with shape {y.shape}\")\n",
    "    \n",
    "    # Define models and train\n",
    "    models = define_models_hyperparameters()\n",
    "    print(\"Starting model training and evaluation...\")\n",
    "    results = train_evaluate_models_with_grid_search(X, y, models, cv=3)\n",
    "    \n",
    "    # Save results\n",
    "    save_results_to_csv(results)\n",
    "    \n",
    "    # Save Word2Vec model\n",
    "    model.save(\"word2vec_model_full_stack_sequence.model\")\n",
    "    print(\"Word2Vec model saved as 'word2vec_model_full_stack_sequence.model'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data with 29560 samples.\n",
      "Parsed sequences from columns.\n",
      "Trained Word2Vec model.\n",
      "Aggregated embeddings for each sample.\n",
      "The following specified molecule columns are not in the dataframe and will be skipped: ['(nan)', '(BE)', '(HDA)']\n",
      "Dropped 624 samples due to NaN in target 'JV_default_PCE'.\n",
      "Prepared feature matrix with shape (28936, 221) and target vector with shape (28936,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Load and Filter Data\n",
    "def load_and_filter_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the specified CSV file and filters rows where 'Cell_architecture' is 'nip'.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data[data['Cell_architecture'].str.strip().str.lower() == 'nip']\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "# 2. Define Layer Columns\n",
    "def define_layer_columns():\n",
    "    \"\"\"\n",
    "    Defines the mapping between stack sequence columns and their corresponding layer names.\n",
    "    \"\"\"\n",
    "    layer_columns = {\n",
    "        'Cell_stack_sequence': 'Cell',\n",
    "        'Substrate_stack_sequence': 'Substrate',\n",
    "        'ETL_stack_sequence': 'ETL',\n",
    "        'HTL_stack_sequence': 'HTL',\n",
    "        'Backcontact_stack_sequence': 'Backcontact',\n",
    "        'Add_lay_back_stack_sequence': 'Add_Lay_Back',\n",
    "        'Encapsulation_stack_sequence': 'Encapsulation'\n",
    "    }\n",
    "    return layer_columns\n",
    "\n",
    "# 3. Parse Sequences from Multiple Columns\n",
    "def parse_sequences_from_columns(dataframe, layer_columns):\n",
    "    \"\"\"\n",
    "    Parses material sequences from multiple layer-specific columns and maps materials to their layers.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    material_layer_map = {}\n",
    "    layer_names = list(layer_columns.values())\n",
    "    \n",
    "    for idx, row in dataframe.iterrows():\n",
    "        sequence = []\n",
    "        for col, layer_name in layer_columns.items():\n",
    "            seq_str = row.get(col, \"\")\n",
    "            if pd.isna(seq_str) or not seq_str.strip():\n",
    "                continue\n",
    "            sub_layers = seq_str.split(' | ')\n",
    "            for sub_layer in sub_layers:\n",
    "                materials = [material.strip() for material in sub_layer.split('; ') if material.strip()]\n",
    "                sequence.extend(materials)\n",
    "                for material in materials:\n",
    "                    if material not in material_layer_map:\n",
    "                        material_layer_map[material] = {}\n",
    "                    if layer_name not in material_layer_map[material]:\n",
    "                        material_layer_map[material][layer_name] = 0\n",
    "                    material_layer_map[material][layer_name] += 1\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return sequences, material_layer_map, layer_names\n",
    "\n",
    "# 4. Train Word2Vec Model\n",
    "def train_word2vec(sequences, vector_size=50, window=5, min_count=1, workers=4, sg=1):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the provided material sequences.\n",
    "    \"\"\"\n",
    "    model = Word2Vec(\n",
    "        sentences=sequences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 5. Prepare Features and Targets\n",
    "def prepare_features_targets(aggregated_features, dataframe, target_column='JV_default_PCE'):\n",
    "    \"\"\"\n",
    "    Prepares the feature matrix and target vector for model training.\n",
    "    Now includes embeddings and specified molecule columns.\n",
    "    \"\"\"\n",
    "    # Molecule columns as a list\n",
    "    molecule_columns = [\n",
    "        '(DAP)', '(PEI)', '(ThFA)', 'Sn', 'S', 'Tb', 'Sm', 'TN', '(PPA)', '(PDMA)', '(FEA)', \n",
    "        '(PyrEA)', 'OA', '(PBA)', '(PTA)', '(CPEA)', '(TEA)', '(mF1PEA)', 'FA', '(BI)', 'IM', \n",
    "        '(oF1PEA)', '(PA)', '(iPA)', 'Mg', 'Y', 'PR', '(PF6)', '(ODA)', 'F', 'BU', '(Ada)', \n",
    "        'Ca', 'NEA', '(SCN)', '(N-EtPy)', 'HA', '(MIC1)', 'Br', '(AVA)', '((CH3)3S)', '(BIM)', \n",
    "        'Mn', 'MA', '(4AMP)', '(A43)', '(CH3)3S', '(PPEA)', '(F5PEA)', '(C4H9N2H6)', '(5-AVAI)', \n",
    "        'Sr', '(DMA)', 'CA', 'Al', '(NH4)', '(4AMPY)', 'PN', 'Sb', '(PDA)', '(ALA)', 'Nb', 'Te', \n",
    "        'TA', '(MTEA)', '(Cl-PEA)', '(iso-BA)', '(DPA)', '(BYA)', 'DA', 'Bi', '(HTAB)', 'AN', \n",
    "        'NMABr', '(CHMA)', '(F3EA)', 'In', '(6-ACA)', 'GU', '(ImEA)', '(HEA)', 'IA', 'Aa', \n",
    "        '(APMim)', '(C8H17NH3)', '(Br-PEA)', 'PMA', '(MIC2)', '(PGA)', 'I', '(5-AVA)', '(PEA)', \n",
    "        'K', '(BEA)', '(PMA)', 'Eu', 'Cl', '(3AMP)', '(F-PEA)', '(C6H4NH2)', '(CH3ND3)', '(4FPEA)', \n",
    "        '(DAT)', '(Anyl)', '(TBA)', '(4ApyH)', 'Ba', '(pF1PEA)', '(TMA)', 'Rb', '(3AMPY)', '(IEA)', \n",
    "        '(nan)', '(NMA)', 'Ni', '(pFPEA)', '(BE)', '(EU-pyP)', '(PyEA)', '(BzDA)', 'Co', '(Ace)', \n",
    "        'Hg', 'Pb', '(EDA)', '(oFPEA)', 'Bn', '(f-PEA)', '(C4H9NH3)', '(CIEA)', '(mFPEA)', 'BA', \n",
    "        'DI', '(HdA)', '(PDA)', '(GABA)', 'Cu', 'PA', '(DMA)', 'Na', '(EPA)', '(OdA)', '(THM)', \n",
    "        'Ge', '(HDA)', '(BF4)', '(FPEA)', '(MIC3)', 'GA', '(ThMA)', 'Cs', '(BZA)', 'Au', '(H-PEA)', \n",
    "        'Ag', '(SCN)', '(TFEA)', 'EA', 'FPEAI', 'Fe', '(n-C3H7NH3)', '(BdA)', '(EDA)', 'BDA', 'Cr', \n",
    "        'Pt', 'Ti', '(C6H13NH3)', '(HAD)', 'Li', '(BDA)', 'O', 'La', 'Zn'\n",
    "    ]\n",
    "    \n",
    "    # Keep only columns that exist in the dataframe\n",
    "    existing_molecule_columns = [col for col in molecule_columns if col in dataframe.columns]\n",
    "    missing_columns = [col for col in molecule_columns if col not in dataframe.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"The following specified molecule columns are not in the dataframe and will be skipped: {missing_columns}\")\n",
    "    \n",
    "    # Extract the specified molecule columns from the dataframe\n",
    "    molecule_features = dataframe[existing_molecule_columns]\n",
    "    \n",
    "    # Handle missing values in molecule features\n",
    "    molecule_features = molecule_features.fillna(0.0)\n",
    "    \n",
    "    # Combine embeddings and molecule features\n",
    "    combined_features = np.hstack([aggregated_features, molecule_features.values])\n",
    "    \n",
    "    # Handle missing values in the combined features\n",
    "    combined_features = np.nan_to_num(combined_features, nan=0.0)\n",
    "    \n",
    "    # Combine features and target into a DataFrame for easier handling\n",
    "    feature_df = pd.DataFrame(combined_features)\n",
    "    target_series = dataframe[target_column]\n",
    "    \n",
    "    # Concatenate features and target\n",
    "    combined_df = pd.concat([feature_df, target_series], axis=1)\n",
    "    \n",
    "    # Drop rows where target is NaN\n",
    "    initial_count = combined_df.shape[0]\n",
    "    combined_df = combined_df.dropna(subset=[target_column])\n",
    "    final_count = combined_df.shape[0]\n",
    "    dropped = initial_count - final_count\n",
    "    if dropped > 0:\n",
    "        print(f\"Dropped {dropped} samples due to NaN in target '{target_column}'.\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = combined_df.drop(columns=[target_column]).values\n",
    "    y = combined_df[target_column].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 6. Main Execution Function\n",
    "def main():\n",
    "    # File path to the CSV dataset\n",
    "    file_path = 'data_with_layer_type_and_combined.csv'\n",
    "    \n",
    "    # Load and filter data\n",
    "    data = load_and_filter_data(file_path)\n",
    "    print(f\"Loaded data with {data.shape[0]} samples.\")\n",
    "    \n",
    "    # Define layer columns\n",
    "    layer_columns = define_layer_columns()\n",
    "    \n",
    "    # Parse sequences\n",
    "    tokenized_sequences, material_layer_map, layer_names = parse_sequences_from_columns(data, layer_columns)\n",
    "    print(\"Parsed sequences from columns.\")\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    model = train_word2vec(tokenized_sequences)\n",
    "    print(\"Trained Word2Vec model.\")\n",
    "    \n",
    "    # Aggregate embeddings\n",
    "    aggregated_features = aggregate_embeddings(tokenized_sequences, model)\n",
    "    print(\"Aggregated embeddings for each sample.\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X, y = prepare_features_targets(aggregated_features, data)\n",
    "    print(f\"Prepared feature matrix with shape {X.shape} and target vector with shape {y.shape}\")\n",
    "    \n",
    "    # Model training logic can be added here\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data with 29560 samples.\n",
      "Parsed sequences from columns.\n",
      "Trained Word2Vec model.\n",
      "Aggregated embeddings for each sample.\n",
      "The following specified molecule columns are not in the dataframe and will be skipped: ['(nan)', '(BE)', '(HDA)']\n",
      "Dropped 624 samples due to NaN in target 'JV_default_PCE'.\n",
      "Prepared feature matrix with shape (28936, 221) and target vector with shape (28936,).\n",
      "Training RandomForest...\n",
      "RandomForest Best Params: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "MAE: 2.8973, MSE: 14.2478, R2: 0.4960\n",
      "Training GradientBoosting...\n",
      "GradientBoosting Best Params: {'learning_rate': 0.1, 'max_depth': 4, 'min_samples_split': 4, 'n_estimators': 150}\n",
      "MAE: 2.9656, MSE: 14.5315, R2: 0.4859\n",
      "Results saved to model_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Load and Filter Data\n",
    "def load_and_filter_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the specified CSV file and filters rows where 'Cell_architecture' is 'nip'.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data[data['Cell_architecture'].str.strip().str.lower() == 'nip']\n",
    "    data = data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "# 2. Define Layer Columns\n",
    "def define_layer_columns():\n",
    "    \"\"\"\n",
    "    Defines the mapping between stack sequence columns and their corresponding layer names.\n",
    "    \"\"\"\n",
    "    layer_columns = {\n",
    "        'Cell_stack_sequence': 'Cell',\n",
    "        'Substrate_stack_sequence': 'Substrate',\n",
    "        'ETL_stack_sequence': 'ETL',\n",
    "        'HTL_stack_sequence': 'HTL',\n",
    "        'Backcontact_stack_sequence': 'Backcontact',\n",
    "        'Add_lay_back_stack_sequence': 'Add_Lay_Back',\n",
    "        'Encapsulation_stack_sequence': 'Encapsulation'\n",
    "    }\n",
    "    return layer_columns\n",
    "\n",
    "# 3. Parse Sequences from Multiple Columns\n",
    "def parse_sequences_from_columns(dataframe, layer_columns):\n",
    "    \"\"\"\n",
    "    Parses material sequences from multiple layer-specific columns and maps materials to their layers.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    material_layer_map = {}\n",
    "    layer_names = list(layer_columns.values())\n",
    "    \n",
    "    for idx, row in dataframe.iterrows():\n",
    "        sequence = []\n",
    "        for col, layer_name in layer_columns.items():\n",
    "            seq_str = row.get(col, \"\")\n",
    "            if pd.isna(seq_str) or not seq_str.strip():\n",
    "                continue\n",
    "            sub_layers = seq_str.split(' | ')\n",
    "            for sub_layer in sub_layers:\n",
    "                materials = [material.strip() for material in sub_layer.split('; ') if material.strip()]\n",
    "                sequence.extend(materials)\n",
    "                for material in materials:\n",
    "                    if material not in material_layer_map:\n",
    "                        material_layer_map[material] = {}\n",
    "                    if layer_name not in material_layer_map[material]:\n",
    "                        material_layer_map[material][layer_name] = 0\n",
    "                    material_layer_map[material][layer_name] += 1\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return sequences, material_layer_map, layer_names\n",
    "\n",
    "# 4. Train Word2Vec Model\n",
    "def train_word2vec(sequences, vector_size=50, window=5, min_count=1, workers=4, sg=1):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the provided material sequences.\n",
    "    \"\"\"\n",
    "    model = Word2Vec(\n",
    "        sentences=sequences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 5. Aggregate Embeddings for Each Sample\n",
    "def aggregate_embeddings(sequences, model, vector_size=50):\n",
    "    \"\"\"\n",
    "    Aggregates material embeddings for each sample by averaging.\n",
    "    \"\"\"\n",
    "    aggregated_features = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) == 0:\n",
    "            aggregated_features.append(np.zeros(vector_size))\n",
    "            continue\n",
    "        vectors = [model.wv[material] for material in seq if material in model.wv]\n",
    "        if vectors:\n",
    "            aggregated = np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            aggregated = np.zeros(vector_size)\n",
    "        aggregated_features.append(aggregated)\n",
    "    return np.array(aggregated_features)\n",
    "\n",
    "# 6. Prepare Features and Targets\n",
    "def prepare_features_targets(aggregated_features, dataframe, target_column='JV_default_PCE'):\n",
    "    \"\"\"\n",
    "    Prepares the feature matrix and target vector for model training.\n",
    "    Includes embeddings and specified molecule columns.\n",
    "    \"\"\"\n",
    "    # Molecule columns as a list\n",
    "    molecule_columns = [\n",
    "        '(DAP)', '(PEI)', '(ThFA)', 'Sn', 'S', 'Tb', 'Sm', 'TN', '(PPA)', '(PDMA)', '(FEA)', \n",
    "        '(PyrEA)', 'OA', '(PBA)', '(PTA)', '(CPEA)', '(TEA)', '(mF1PEA)', 'FA', '(BI)', 'IM', \n",
    "        '(oF1PEA)', '(PA)', '(iPA)', 'Mg', 'Y', 'PR', '(PF6)', '(ODA)', 'F', 'BU', '(Ada)', \n",
    "        'Ca', 'NEA', '(SCN)', '(N-EtPy)', 'HA', '(MIC1)', 'Br', '(AVA)', '((CH3)3S)', '(BIM)', \n",
    "        'Mn', 'MA', '(4AMP)', '(A43)', '(CH3)3S', '(PPEA)', '(F5PEA)', '(C4H9N2H6)', '(5-AVAI)', \n",
    "        'Sr', '(DMA)', 'CA', 'Al', '(NH4)', '(4AMPY)', 'PN', 'Sb', '(PDA)', '(ALA)', 'Nb', 'Te', \n",
    "        'TA', '(MTEA)', '(Cl-PEA)', '(iso-BA)', '(DPA)', '(BYA)', 'DA', 'Bi', '(HTAB)', 'AN', \n",
    "        'NMABr', '(CHMA)', '(F3EA)', 'In', '(6-ACA)', 'GU', '(ImEA)', '(HEA)', 'IA', 'Aa', \n",
    "        '(APMim)', '(C8H17NH3)', '(Br-PEA)', 'PMA', '(MIC2)', '(PGA)', 'I', '(5-AVA)', '(PEA)', \n",
    "        'K', '(BEA)', '(PMA)', 'Eu', 'Cl', '(3AMP)', '(F-PEA)', '(C6H4NH2)', '(CH3ND3)', '(4FPEA)', \n",
    "        '(DAT)', '(Anyl)', '(TBA)', '(4ApyH)', 'Ba', '(pF1PEA)', '(TMA)', 'Rb', '(3AMPY)', '(IEA)', \n",
    "        '(nan)', '(NMA)', 'Ni', '(pFPEA)', '(BE)', '(EU-pyP)', '(PyEA)', '(BzDA)', 'Co', '(Ace)', \n",
    "        'Hg', 'Pb', '(EDA)', '(oFPEA)', 'Bn', '(f-PEA)', '(C4H9NH3)', '(CIEA)', '(mFPEA)', 'BA', \n",
    "        'DI', '(HdA)', '(PDA)', '(GABA)', 'Cu', 'PA', '(DMA)', 'Na', '(EPA)', '(OdA)', '(THM)', \n",
    "        'Ge', '(HDA)', '(BF4)', '(FPEA)', '(MIC3)', 'GA', '(ThMA)', 'Cs', '(BZA)', 'Au', '(H-PEA)', \n",
    "        'Ag', '(SCN)', '(TFEA)', 'EA', 'FPEAI', 'Fe', '(n-C3H7NH3)', '(BdA)', '(EDA)', 'BDA', 'Cr', \n",
    "        'Pt', 'Ti', '(C6H13NH3)', '(HAD)', 'Li', '(BDA)', 'O', 'La', 'Zn'\n",
    "    ]\n",
    "    \n",
    "    # Keep only columns that exist in the dataframe\n",
    "    existing_molecule_columns = [col for col in molecule_columns if col in dataframe.columns]\n",
    "    missing_columns = [col for col in molecule_columns if col not in dataframe.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"The following specified molecule columns are not in the dataframe and will be skipped: {missing_columns}\")\n",
    "    \n",
    "    # Extract the specified molecule columns from the dataframe\n",
    "    molecule_features = dataframe[existing_molecule_columns]\n",
    "    \n",
    "    # Handle missing values in molecule features\n",
    "    molecule_features = molecule_features.fillna(0.0)\n",
    "    \n",
    "    # Combine embeddings and molecule features\n",
    "    combined_features = np.hstack([aggregated_features, molecule_features.values])\n",
    "    \n",
    "    # Handle missing values in the combined features\n",
    "    combined_features = np.nan_to_num(combined_features, nan=0.0)\n",
    "    \n",
    "    # Combine features and target into a DataFrame for easier handling\n",
    "    feature_df = pd.DataFrame(combined_features)\n",
    "    target_series = dataframe[target_column]\n",
    "    \n",
    "    # Concatenate features and target\n",
    "    combined_df = pd.concat([feature_df, target_series], axis=1)\n",
    "    \n",
    "    # Drop rows where target is NaN\n",
    "    initial_count = combined_df.shape[0]\n",
    "    combined_df = combined_df.dropna(subset=[target_column])\n",
    "    final_count = combined_df.shape[0]\n",
    "    dropped = initial_count - final_count\n",
    "    if dropped > 0:\n",
    "        print(f\"Dropped {dropped} samples due to NaN in target '{target_column}'.\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = combined_df.drop(columns=[target_column]).values\n",
    "    y = combined_df[target_column].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 7. Train and Evaluate Models\n",
    "def train_evaluate_models_with_grid_search(X, y, cv=3):\n",
    "    \"\"\"\n",
    "    Trains and evaluates models using GridSearchCV for hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'RandomForest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 150],\n",
    "                'max_depth': [None, 10],\n",
    "                'min_samples_split': [2, 4],\n",
    "                'min_samples_leaf': [1, 2]\n",
    "            }\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'model': GradientBoostingRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 150],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'max_depth': [3, 4],\n",
    "                'min_samples_split': [2, 4]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    results = []\n",
    "    \n",
    "    for model_name, config in models.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=config['model'],\n",
    "            param_grid=config['params'],\n",
    "            cv=cv,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X, y)\n",
    "        \n",
    "        best_params = grid_search.best_params_\n",
    "        best_estimator = grid_search.best_estimator_\n",
    "        y_pred = best_estimator.predict(X)\n",
    "        \n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Parameters': best_params,\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'R2': r2\n",
    "        })\n",
    "        \n",
    "        print(f\"{model_name} Best Params: {best_params}\")\n",
    "        print(f\"MAE: {mae:.4f}, MSE: {mse:.4f}, R2: {r2:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 8. Save Results\n",
    "def save_results_to_csv(results, filename='model_results.csv'):\n",
    "    \"\"\"\n",
    "    Saves the model training results to a CSV file.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to save.\")\n",
    "        return\n",
    "    \n",
    "    keys = results[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(results)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "# 9. Main Execution Function\n",
    "def main():\n",
    "    # File path to the CSV dataset\n",
    "    file_path = 'data_with_layer_type_and_combined.csv'\n",
    "    \n",
    "    # Load and filter data\n",
    "    data = load_and_filter_data(file_path)\n",
    "    print(f\"Loaded data with {data.shape[0]} samples.\")\n",
    "    \n",
    "    # Define layer columns\n",
    "    layer_columns = define_layer_columns()\n",
    "    \n",
    "    # Parse sequences\n",
    "    tokenized_sequences, material_layer_map, layer_names = parse_sequences_from_columns(data, layer_columns)\n",
    "    print(\"Parsed sequences from columns.\")\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    model = train_word2vec(tokenized_sequences)\n",
    "    print(\"Trained Word2Vec model.\")\n",
    "    \n",
    "    # Aggregate embeddings\n",
    "    aggregated_features = aggregate_embeddings(tokenized_sequences, model)\n",
    "    print(\"Aggregated embeddings for each sample.\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X, y = prepare_features_targets(aggregated_features, data)\n",
    "    print(f\"Prepared feature matrix with shape {X.shape} and target vector with shape {y.shape}.\")\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    results = train_evaluate_models_with_grid_search(X, y)\n",
    "    \n",
    "    # Save results\n",
    "    save_results_to_csv(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perovskite_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
